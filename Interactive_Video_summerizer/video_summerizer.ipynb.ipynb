{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45035,"status":"ok","timestamp":1730431369076,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"pAXkd2fG-60F","outputId":"30f2b11f-4582-4d48-b870-6a56d5ed6281"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1730431369077,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"Fnjzcg2h_Lpj","outputId":"40797bee-e37a-48db-ef92-4245b9369f14"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Internships2024/Al-Zira-Internship/Interactive_Video_Summerizer\n"]}],"source":["cd /content/drive/MyDrive/Colab Notebooks/Internships2024/Al-Zira-Internship/Interactive_Video_Summerizer"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1730431369077,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"8v2hpLQY_LsB"},"outputs":[],"source":["mainPath=r\"/content/drive/MyDrive/Colab Notebooks/Internships2024/Al-Zira-Internship/Interactive_Video_Summerizer\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7581,"status":"ok","timestamp":1730301831806,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"IHuTEFO0Bh9Y","outputId":"11bd2bf8-ea74-470e-8bc2-8d9e2f9a3e07"},"outputs":[{"name":"stdout","output_type":"stream","text":["\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n","\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connected to cloud.r-project.org (18.239.18.61\r                                                                                                    \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n","\r0% [Waiting for headers] [Connecting to r2u.stat.illinois.edu (192.17.190.167)] [Waiting for headers\r                                                                                                    \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] [Waiting for headers]\r                                                                                                    \rHit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] [Waiting for headers]\r                                                                                                    \rHit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] [Waiting for headers]\r                                                                                                    \rHit:6 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:7 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n","Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Ign:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Hit:11 https://r2u.stat.illinois.edu/ubuntu jammy Release\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 84 not upgraded.\n"]}],"source":["!apt-get update\n","!apt-get install ffmpeg"]},{"cell_type":"markdown","metadata":{"id":"8qSRU8X1_UhT"},"source":["## VIdio to Audio\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuJd7E89_Lur"},"outputs":[],"source":["from moviepy.editor import AudioFileClip\n","\n","def split_audio_into_equal_chunks(audio_file_path, chunk_duration):\n","    # Load the audio file\n","    audio = AudioFileClip(audio_file_path)\n","\n","    # Get the total duration of the audio file in seconds\n","    total_duration = audio.duration\n","\n","    # Calculate the number of chunks\n","    num_chunks = int(total_duration // chunk_duration)\n","\n","    # Split the audio into equal chunks\n","    for i in range(num_chunks):\n","        start_time = i * chunk_duration\n","        end_time = start_time + chunk_duration\n","\n","        # Create a subclip for the chunk\n","        audio_chunk = audio.subclip(start_time, end_time)\n","\n","        # Write the chunk to a file\n","        audio_chunk.write_audiofile(f\"chunk_{i + 1}.mp3\")  # Change the format as needed\n","\n","    # Handle any remaining audio that doesn't fit into a full chunk\n","    if total_duration % chunk_duration > 0:\n","        start_time = num_chunks * chunk_duration\n","        audio_chunk = audio.subclip(start_time, total_duration)\n","        audio_chunk.write_audiofile(f\"chunk_{num_chunks + 1}.mp3\")  # Last chunk for remaining audio\n","\n","    # Close the audio file\n","    audio.close()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45469,"status":"ok","timestamp":1730302242119,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"AxEB8DdS_Lw5","outputId":"a0136858-c8aa-4864-b65d-92a5731789b4"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:34<1:40:00,  9.57s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Writing audio in chunk_1.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:36<1:40:32,  9.62s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:36<1:40:33,  9.62s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_2.mp3\n"]},{"name":"stderr","output_type":"stream","text":["\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:39<1:41:23,  9.70s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:39<1:41:23,  9.70s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_3.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:40<1:41:48,  9.74s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:40<1:41:48,  9.74s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_4.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:42<1:42:09,  9.78s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:42<1:42:10,  9.78s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_5.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:43<1:42:26,  9.80s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:43<1:42:26,  9.80s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_6.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:43<1:42:39,  9.82s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:43<1:42:39,  9.82s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_7.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                    \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:44<1:42:52,  9.84s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:44<1:42:52,  9.84s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_8.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:45<1:43:06,  9.87s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:45<1:43:06,  9.87s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_9.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                    \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:46<1:43:18,  9.89s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:46<1:43:19,  9.89s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_10.mp3\n"]},{"name":"stderr","output_type":"stream","text":["\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:46<1:43:32,  9.91s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:46<1:43:32,  9.91s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_11.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                    \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:47<1:43:45,  9.93s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:47<1:43:45,  9.93s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_12.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:48<1:43:59,  9.95s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:48<1:43:59,  9.95s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_13.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:48<1:44:12,  9.97s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:49<1:44:12,  9.97s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_14.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:49<1:44:28, 10.00s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:49<1:44:28, 10.00s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_15.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:50<1:44:47, 10.03s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:51<1:44:47, 10.03s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_16.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:51<1:45:01, 10.05s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:51<1:45:01, 10.05s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_17.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                    \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:52<1:45:13, 10.07s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:52<1:45:13, 10.07s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_18.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:53<1:45:33, 10.10s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:53<1:45:33, 10.10s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_19.mp3\n"]},{"name":"stderr","output_type":"stream","text":["\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:54<1:45:54, 10.13s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:54<1:45:54, 10.14s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_20.mp3\n"]},{"name":"stderr","output_type":"stream","text":["\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:55<1:46:15, 10.17s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:55<1:46:16, 10.17s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_21.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:57<1:46:37, 10.20s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:57<1:46:37, 10.20s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_22.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:58<1:46:59, 10.24s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:58<1:47:00, 10.24s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_23.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:59<1:47:19, 10.27s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [05:59<1:47:20, 10.27s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_24.mp3\n"]},{"name":"stderr","output_type":"stream","text":["\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:00<1:47:33, 10.29s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:00<1:47:33, 10.29s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_25.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:00<1:47:46, 10.31s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:00<1:47:46, 10.31s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_26.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:01<1:48:00, 10.34s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:01<1:48:00, 10.34s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_27.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                    \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:02<1:48:13, 10.36s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:02<1:48:13, 10.36s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_28.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:03<1:48:27, 10.38s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:03<1:48:27, 10.38s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_29.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                    \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:03<1:48:39, 10.40s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:03<1:48:40, 10.40s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_30.mp3\n"]},{"name":"stderr","output_type":"stream","text":["\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:04<1:48:53, 10.42s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:04<1:48:53, 10.42s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_31.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                    \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:05<1:49:06, 10.44s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:05<1:49:06, 10.44s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_32.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:06<1:49:19, 10.46s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:06<1:49:19, 10.46s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_33.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                    \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:06<1:49:32, 10.48s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:06<1:49:32, 10.48s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_34.mp3\n"]},{"name":"stderr","output_type":"stream","text":["\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:07<1:49:45, 10.50s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:07<1:49:45, 10.50s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_35.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:08<1:49:59, 10.52s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:08<1:49:59, 10.53s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_36.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                    \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:09<1:50:11, 10.55s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:09<1:50:11, 10.55s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_37.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:10<1:50:31, 10.58s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:10<1:50:32, 10.58s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_38.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:11<1:50:52, 10.61s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:11<1:50:53, 10.61s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_39.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:12<1:51:15, 10.65s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:12<1:51:15, 10.65s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_40.mp3\n"]},{"name":"stderr","output_type":"stream","text":["\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:13<1:51:36, 10.68s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:13<1:51:36, 10.68s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_41.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:15<1:51:59, 10.72s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:15<1:51:59, 10.72s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_42.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:16<1:52:17, 10.75s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:16<1:52:17, 10.75s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_43.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:16<1:52:30, 10.77s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:16<1:52:31, 10.77s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_44.mp3\n"]},{"name":"stderr","output_type":"stream","text":["\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:17<1:52:44, 10.79s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:17<1:52:44, 10.79s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_45.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:18<1:52:57, 10.81s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:18<1:52:57, 10.81s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_46.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:19<1:53:12, 10.83s/it, now=None]\u001b[A\n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:19<1:53:13, 10.83s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","MoviePy - Writing audio in chunk_47.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                  \n","\u001b[A\n","chunk:   5%|▌         | 35/662 [06:19<1:53:17, 10.84s/it, now=None]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n"]}],"source":["split_audio_into_equal_chunks(mainPath+'/data/Pandas for Data Science in 20 Minutes _ Python Crash Course.wav',30)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1730302377286,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"fX71b3rzDkfL","outputId":"17744a23-b979-4826-95e6-1eefb3b41bf1"},"outputs":[{"data":{"text/plain":["['data',\n"," 'progress.gdoc',\n"," 'experimenting.ipynb',\n"," 'video_summerizer.ipynb.ipynb',\n"," 'chunk_2TEMP_MPY_wvf_snd.mp3',\n"," 'chunk_1.mp3',\n"," 'chunk_2.mp3',\n"," 'chunk_3.mp3',\n"," 'chunk_4.mp3',\n"," 'chunk_5.mp3',\n"," 'chunk_6.mp3',\n"," 'chunk_7.mp3',\n"," 'chunk_8.mp3',\n"," 'chunk_9.mp3',\n"," 'chunk_10.mp3',\n"," 'chunk_11.mp3',\n"," 'chunk_12.mp3',\n"," 'chunk_13.mp3',\n"," 'chunk_14.mp3',\n"," 'chunk_15.mp3',\n"," 'chunk_16.mp3',\n"," 'chunk_17.mp3',\n"," 'chunk_18.mp3',\n"," 'chunk_19.mp3',\n"," 'chunk_20.mp3',\n"," 'chunk_21.mp3',\n"," 'chunk_22.mp3',\n"," 'chunk_23.mp3',\n"," 'chunk_24.mp3',\n"," 'chunk_25.mp3',\n"," 'chunk_26.mp3',\n"," 'chunk_27.mp3',\n"," 'chunk_28.mp3',\n"," 'chunk_29.mp3',\n"," 'chunk_30.mp3',\n"," 'chunk_31.mp3',\n"," 'chunk_32.mp3',\n"," 'chunk_33.mp3',\n"," 'chunk_34.mp3',\n"," 'chunk_35.mp3',\n"," 'chunk_36.mp3',\n"," 'chunk_37.mp3',\n"," 'chunk_38.mp3',\n"," 'chunk_39.mp3',\n"," 'chunk_40.mp3',\n"," 'chunk_41.mp3',\n"," 'chunk_42.mp3',\n"," 'chunk_43.mp3',\n"," 'chunk_44.mp3',\n"," 'chunk_45.mp3',\n"," 'chunk_46.mp3',\n"," 'chunk_47.mp3']"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","os.listdir(os.getcwd())"]},{"cell_type":"markdown","metadata":{"id":"JG201SL1_y0G"},"source":["## Audio to Transcript"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32037,"status":"ok","timestamp":1730431426110,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"T9tG12ZE_LzM","outputId":"be213d4b-278d-4e23-b39b-a2bd4801a701"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-e8uf5_vk\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-e8uf5_vk\n","  Resolved https://github.com/openai/whisper.git to commit 5979f03701209bb035a0a466f14131aeb1116cbb\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.0+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.66.6)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n","Collecting tiktoken (from openai-whisper==20240930)\n","  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Collecting triton>=2.0.0 (from openai-whisper==20240930)\n","  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20240930) (3.16.1)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.9.11)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n","Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803560 sha256=0fc87f7ff08f27f4c0771da6e934a542e7a3a676c571d03bae207e43376c6736\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-tpp4gis8/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n","Successfully built openai-whisper\n","Installing collected packages: triton, tiktoken, openai-whisper\n","Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n","Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,103 kB]\n","Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n","Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n","Hit:8 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,389 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,430 kB]\n","Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,162 kB]\n","Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,227 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,606 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,305 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,665 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,450 kB]\n","Fetched 26.7 MB in 3s (8,976 kB/s)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","51 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n"]}],"source":["!pip install git+https://github.com/openai/whisper.git\n","!sudo apt update && sudo apt install ffmpeg"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":11078,"status":"error","timestamp":1730353665324,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"_ou1nQjx_L1p","outputId":"d01d6604-03b5-43d2-d5d2-a6ea933e42cc"},"outputs":[{"name":"stderr","output_type":"stream","text":[" 28%|██████████▍                           | 401M/1.42G [00:04<00:12, 89.1MiB/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-9b38f5539054>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# model = whisper.load_model(\"base\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# small_model = whisper.load_model(\"small\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmedium_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"medium\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# large_model = whisper.load_model(\"large\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_MODELS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mcheckpoint_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MODELS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0malignment_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ALIGNMENT_HEADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/__init__.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(url, root, in_memory)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mmodel_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1240\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ema_dn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ema_dt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_miniters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m                     \u001b[0;31m# If no `miniters` was specified, adjust automatically to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_meter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mformat_meter\u001b[0;34m(n, total, elapsed, ncols, prefix, ascii, unit, unit_scale, rate, bar_format, postfix, unit_divisor, initial, colour, **extra_kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m                           if rate else '?') + unit + '/s'\n\u001b[1;32m    556\u001b[0m         rate_inv_fmt = (\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mformat_sizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_rate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mf'{inv_rate:5.2f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m             if inv_rate else '?') + 's/' + unit\n\u001b[1;32m    559\u001b[0m         \u001b[0mrate_fmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrate_inv_fmt\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minv_rate\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minv_rate\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrate_noinv_fmt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mformat_sizeof\u001b[0;34m(num, suffix, divisor)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \"\"\"\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0munit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'k'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'M'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'G'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'T'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'P'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'E'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Z'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m999.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m99.95\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m9.995\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import whisper\n","# # Load the Whisper model\n","# model = whisper.load_model(\"base\")\n","# small_model = whisper.load_model(\"small\")\n","medium_model = whisper.load_model(\"medium\")\n","# large_model = whisper.load_model(\"large\")"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65543,"status":"ok","timestamp":1730431546853,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"ZW_KVB3REROi","outputId":"13647fd0-90d3-49b0-cdee-213c5b667735"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(fp, map_location=device)\n"]}],"source":["import whisper\n","import os\n","\n","# def transcribe_audio_chunks(audio_folder, model_name=\"base\"):\n","#     # Load the Whisper model\n","model = whisper.load_model(\"base\",device='cuda')\n","\n","#     # List all audio files in the folder\n","#     audio_files = sorted([f for f in os.listdir(audio_folder) if f.endswith('.mp3')])\n","\n","#     # Store transcriptions with timestamps\n","#     all_transcriptions = []\n","\n","#     for i, audio_file in enumerate(audio_files, start=1):\n","#         audio_path = os.path.join(audio_folder,f\"chunk_{i}.mp3\" )\n","#         print(f\"Transcribing {audio_file}...\")\n","\n","#         # Transcribe audio with timestamps\n","#         result = model.transcribe(audio_path, language='en',fp16=False)\n","\n","#         # Extract text and timestamps for each segment\n","#         transcription = {\n","#             \"chunk\": i,\n","#             \"transcription\": [\n","#                 {\"start\": segment[\"start\"], \"end\": segment[\"end\"], \"text\": segment[\"text\"]}\n","#                 for segment in result[\"segments\"]\n","#             ]\n","#         }\n","\n","#         all_transcriptions.append(transcription)\n","\n","#         # Print the transcription with timestamps\n","#         print(f\"Chunk {i} Transcription:\")\n","#         for segment in transcription[\"transcription\"]:\n","#             print(f\"[{segment['start']} - {segment['end']}] {segment['text']}\")\n","\n","#     return all_transcriptions\n","\n","\n","# Example usage\n"," # Replace with your actual output folder path\n","# transcriptions = transcribe_audio_chunks(mainPath, model_name=\"base\")\n","\n","result=model.transcribe(mainPath+'/data/Pandas for Data Science in 20 Minutes _ Python Crash Course.wav',word_timestamps=True)\n","original_transcript=[]\n","# Extract and print sentence-level timestamps and transcription\n","for segment in result['segments']:\n","    stmt=[segment['start'],segment['end'],segment['text'].strip()]\n","    original_transcript.append(stmt)"]},{"cell_type":"code","source":["original_transcript"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3XasJYTVcAM","executionInfo":{"status":"ok","timestamp":1730431553591,"user_tz":-330,"elapsed":1719,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"}},"outputId":"984a1795-19f5-41d3-aede-322864dfaf3d"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0.0, 2.0, \"So today we're going to be taking a look at pandas.\"],\n"," [12.540000000000004, 14.44, 'No, not those kinds of pandas.'],\n"," [14.76, 15.74, 'Pandas for data science.'],\n"," [16.24, 19.36, \"So pandas is a ridiculously powerful library that's used\"],\n"," [19.36, 21.34, 'all around the world for data science.'],\n"," [21.66, 24.26, 'It helps you work, process, and filter tabular data'],\n"," [24.26,\n","  27.68,\n","  'so you can get your machine learning models out there a whole heap faster.'],\n"," [27.68,\n","  32.1,\n","  \"And it's probably one of the most useful libraries to have under your belt as a data scientist.\"],\n"," [32.46,\n","  34.86,\n","  \"Let's take a look in detail as to what we're going to be going to do.\"],\n"," [35.02, 36.44, \"So in this video we're going to be covering\"],\n"," [36.44, 37.62, 'crowd for pandas.'],\n"," [37.68, 39.38, \"Now you're probably thinking what the hell is crowd?\"],\n"," [39.5, 42.18, 'Well, it stands for create, read, update, and delete.'],\n"," [42.62,\n","  45.8,\n","  \"We're basically going to be covering everything you need to get up and started\"],\n"," [45.8, 49.0, 'really, really quickly as a data scientist using pandas.'],\n"," [49.2,\n","  51.7,\n","  \"We're also going to be treating this like a bit of a crash course.\"],\n"," [51.78,\n","  55.58,\n","  \"So we'll read in some tabular data and we'll go through all the steps that you need\"],\n"," [55.58, 57.12, 'in order to work with pandas.'],\n"," [57.12, 58.76, \"So in terms of how we're going to be doing this,\"],\n"," [58.78, 61.22, \"we're going to be working with Python and specifically,\"],\n"," [61.22,\n","  66.0,\n","  \"we're going to be using the pandas library and we're going to be coding inside of a\"],\n"," [66.0,\n","  69.78,\n","  \"Jupyter notebook because that's what data scientists all around the world are using.\"],\n"," [70.26, 71.36, \"So you're ready to get into it?\"],\n"," [71.64, 72.32, \"Let's do it.\"],\n"," [72.44, 74.18, \"All right, so let's dive straight into it.\"],\n"," [74.28,\n","  77.26,\n","  \"So in this video, we're going to go through everything you need to get up and\"],\n"," [77.26, 80.04, 'started super fast to work with pandas for data science.'],\n"," [80.42, 82.8, \"So we're going to break up the video into four key sections.\"],\n"," [82.8, 84.68, 'So creating data frames,'],\n"," [84.68,\n","  88.5,\n","  \"reading data frames and specifically, we're going to be taking a look at how to work with all\"],\n"," [88.5, 89.4, 'the data within them.'],\n"," [89.74, 92.14, \"We're then going to be taking a look at how to update them.\"],\n"," [92.26, 94.0, 'So dropping rows and working with columns.'],\n"," [94.62,\n","  98.76,\n","  \"And last but not least, we're going to take a look at how we can output all of our resulting\"],\n"," [98.76, 99.24, 'data.'],\n"," [99.76,\n","  103.08,\n","  \"So the first thing that we're going to want to do is import pandas.\"],\n"," [103.18,\n","  106.96,\n","  'Now we can do that pretty easily using the standard Python import functionality.'],\n"," [108.26,\n","  111.58,\n","  \"All right, so we've imported pandas using the import function.\"],\n"," [111.58, 113.34, 'So import pandas as PD.'],\n"," [113.34,\n","  119.52,\n","  \"Now if we open up our pandas library and hit tab, you can see that we've got a whole bunch\"],\n"," [119.52, 120.72, 'of stuff to work with there.'],\n"," [121.02, 122.24, \"We're not going to go through all of this.\"],\n"," [122.3,\n","  126.74,\n","  \"We're going to really focus on the 20% that's most important as a data scientist.\"],\n"," [127.46, 129.88, 'And probably the most important function is read CSV.'],\n"," [130.24,\n","  134.9,\n","  'So this allows us to create a data frame from a CSV and work with it in pandas.'],\n"," [135.2,\n","  139.86,\n","  \"So in this case, we're going to be using read CSV to open up our telco-churn CSV here.\"],\n"," [140.02, 140.94, 'So if we open this up,'],\n"," [143.34,\n","  148.74,\n","  \"you can see that it's just a regular old CSV with a whole bunch of data.\"],\n"," [148.96,\n","  155.98,\n","  \"And it looks like we've got about 3,334 rows, 3,333 when you exclude our columns.\"],\n"," [156.76, 159.66, \"Let's go ahead and read this in and create a data frame.\"],\n"," [162.46, 163.3, 'All righty.'],\n"," [163.4, 165.24, \"So we've now just created our data frame.\"],\n"," [165.44,\n","  169.98,\n","  'So in order to do that, we use pandas and we used the read CSV function.'],\n"," [170.44, 172.78, 'And then we passed through the name of our CSV.'],\n"," [172.78,\n","  177.52,\n","  \"Now you can't actually see the data frame yet because we haven't actually gone and visualized it.\"],\n"," [177.6,\n","  181.6,\n","  \"Now we're going to skip on over to our read section just for a second to take a look at what\"],\n"," [181.6, 183.06, 'this data frame actually looks like.'],\n"," [183.58,\n","  188.62,\n","  \"In order to visualize our data frame, the easiest function that you're probably going to want to\"],\n"," [188.62, 190.16, 'use is the head function.'],\n"," [190.34,\n","  193.82,\n","  'So this allows you to view the first five rows of data within your data frame.'],\n"," [194.94,\n","  199.0,\n","  \"And you can see that we've got our data frame that we just read in.\"],\n"," [199.0,\n","  204.04,\n","  'Now as I said, the head function allows us to view the first five rows of data.'],\n"," [204.2,\n","  208.66,\n","  'But if we wanted to view more rows from the top, we can just pass through the number that we want to see.'],\n"," [209.04,\n","  214.92,\n","  \"So by passing through 10 to our head function, we're now going to see our first 10 rows of data.\"],\n"," [215.3,\n","  220.5,\n","  'Also just to note when working with pandas, our first row is going to be represented by the index zero.'],\n"," [221.68, 222.4, 'All righty.'],\n"," [222.46,\n","  228.12,\n","  \"So we've now read in data from a CSV and we visualize the first top 10 and top five rows.\"],\n"," [228.12,\n","  232.2,\n","  'Now what we want to do is take a look at how we might create a data frame from a dictionary.'],\n"," [232.58,\n","  237.76,\n","  'So a dictionary is just a different Python data type, but it allows us to work with a bunch of'],\n"," [237.76,\n","  242.4,\n","  \"different methods. Also, when you're working with pandas, you've actually got a whole bunch of\"],\n"," [242.4,\n","  247.72,\n","  'read functions here as well. So if you wanted to read in from your clipboard or Excel or read in from'],\n"," [247.72,\n","  252.98,\n","  \"HTML, SQL, a whole bunch of other stuff, you can do that as well. Here we're going to be creating a\"],\n"," [252.98,\n","  257.36,\n","  'data frame from a dictionary. So what we can do is create a temporary dictionary first.'],\n"," [263.92,\n","  268.34,\n","  'And then what we can do is create a data frame from this dictionary. So in order to do that,'],\n"," [268.38,\n","  273.1,\n","  'we just need to use pandas, access the data frame class and then call from DIT.'],\n"," [276.62,\n","  280.06,\n","  \"You can see we've now created a data frame from a dictionary. So again,\"],\n"," [280.06,\n","  284.76,\n","  \"we can use the head function to visualize it. So this time, we've created a data frame called\"],\n"," [284.76,\n","  290.92,\n","  \"DICDF. So what we can do is visualize that. And there you go. That's our data frame that we've\"],\n"," [290.92,\n","  296.78,\n","  \"created from a dictionary. So let's just quickly recap. So in our create section, we took a look at\"],\n"," [296.78,\n","  301.94,\n","  'how to create a data frame from a CSV. We then also created one from a dictionary. But as I said,'],\n"," [302.1,\n","  307.26,\n","  \"there's a whole bunch of other read functions in pandas that allow you to read in data from different\"],\n"," [307.26,\n","  313.3,\n","  \"data sources. Now, within our read section, what we're going to be taking a look at is how to view\"],\n"," [313.3,\n","  318.14,\n","  \"our top five and bottom five rows. So we've already taken a look at how to read in our top five,\"],\n"," [318.36,\n","  322.78,\n","  \"using the head method. We're also going to take a look at how we can view our columns,\"],\n"," [323.26,\n","  328.24,\n","  'create summary statistics, filtering, and then working with indexes. So using the iLoc and lock'],\n"," [328.24,\n","  334.34,\n","  \"functions. So let's go on ahead and finish up our section 2.1. So one of you are bottom five rows.\"],\n"," [334.34,\n","  338.66,\n","  \"Now that's pretty easy. All we need to do is use the tail method to do that.\"],\n"," [339.82,\n","  344.56,\n","  \"Alrighty. And you can see that we've now used the tail method. And this allows us to view our bottom\"],\n"," [344.56,\n","  349.7,\n","  'five rows. Again, we can pass through a different number of values if we wanted to view a different'],\n"," [349.7,\n","  356.48,\n","  'number of rows. So again, say we wanted to view our bottom 15. Again, this allows us to view our'],\n"," [356.48,\n","  363.96,\n","  'bottom 15 rows. Alrighty. Now on to section 2.2. So viewing columns and different data types,'],\n"," [363.96,\n","  368.02,\n","  \"with pandas you've got a bunch of different columns. So you can see that we've got state,\"],\n"," [368.28,\n","  372.86,\n","  \"account length, so on, and so forth. And we've got enough that we can view on the screen. But when\"],\n"," [372.86,\n","  376.34,\n","  \"you're working with really big data frames, you might want to be able to see all the columns\"],\n"," [376.34,\n","  381.04,\n","  \"that you've got available. To do that, all you need to do is use the columns attribute.\"],\n"," [381.84,\n","  386.56,\n","  \"And you can now see all the columns that we've got within our data frames. So state, voicemail,\"],\n"," [387.0,\n","  392.02,\n","  'total day charge blah blah blah, so on. We can also take a look at the different data types that'],\n"," [392.02,\n","  397.44,\n","  \"we've got available within our data frames using the D type attribute. So within our data frame,\"],\n"," [397.44,\n","  403.02,\n","  \"we've got a number of different data types. So we've got objects, integers, floats. We've also\"],\n"," [403.02,\n","  409.26,\n","  \"got a churn value as an object. Sometimes you'll also see bullions and a bunch of other different\"],\n"," [409.26,\n","  414.28,\n","  \"types of data types. But we've pretty much covered the majority of them here. So this just tells us\"],\n"," [414.28,\n","  418.76,\n","  \"all the different types of data that we've got within our data frame. Now whenever you're working\"],\n"," [418.76,\n","  423.8,\n","  'on a data science project, probably the most important step of the Chris Dan life cycle is'],\n"," [423.8,\n","  428.78,\n","  'understanding your data. A great way to get a handle on your data is to calculate summary statistics.'],\n"," [429.34,\n","  433.74,\n","  'So we can do this using pandas and specifically using the described method.'],\n"," [435.22,\n","  440.18,\n","  \"So using the described method, we're able to calculate our count, our mean, standard deviation,\"],\n"," [441.04,\n","  445.82,\n","  'min, max, as well as our different quartiles. So this gives us a good overview of what our data'],\n"," [445.82,\n","  450.06,\n","  \"actually looks like. But if you pay close attention, when we've called the describe method,\"],\n"," [450.32,\n","  456.24,\n","  \"it's only been called on the data types that are either integers or floats. What happens if we\"],\n"," [456.24,\n","  461.58,\n","  'want to calculate the summary statistics on objects? For example, well, we can do this simply by'],\n"," [461.58,\n","  466.54,\n","  'passing through a different parameter to a described method. So specifically if we call df.describe,'],\n"," [467.0,\n","  473.04,\n","  'what we need to do now is pass through include equals object. So this allows us to calculate'],\n"," [473.04,\n","  480.14,\n","  'summary statistics on our non-flot and integer values. So for state, international plan, voicemail'],\n"," [480.14,\n","  484.22,\n","  \"plan, and churn, we're now able to get our counts, the number of unique values, as well as our\"],\n"," [484.22,\n","  488.84,\n","  \"top values and the frequency. So that pretty much covers summary statistics. Now let's take a look\"],\n"," [488.84,\n","  493.6,\n","  \"at how we can filter on columns. So we took a look before at all the different columns that we've\"],\n"," [493.6,\n","  498.52,\n","  'got. How do we actually filter on the specific columns that we want to visualize? Well, first up,'],\n"," [498.52,\n","  503.76,\n","  \"let's actually take a look at how we can grab a single column. So we've got our state column here.\"],\n"," [504.0,\n","  508.36,\n","  \"How do we go about grabbing that state column? Well, that's pretty easy. All we need to do is access\"],\n"," [508.36,\n","  514.14,\n","  \"that state value. So you can use it like a bit of a key. Now this can get tricky when you've got\"],\n"," [514.14,\n","  518.6,\n","  'keys that have a space in them because if you take a look at international plan for here, for example,'],\n"," [518.84,\n","  525.66,\n","  \"we can't actually type in df.international plan. It's going to throw an error. In order to get\"],\n"," [525.66,\n","  529.44,\n","  'columns that have a space in them, we need to do this a little bit differently. So all we do is pass'],\n"," [529.44,\n","  534.3,\n","  \"through a square bracket and treat it like a key that way. So again, it's doing the same thing.\"],\n"," [534.38,\n","  539.04,\n","  \"It's just a different way to grab a column. So that allows us to grab a single column. Now what\"],\n"," [539.04,\n","  543.72,\n","  \"happens if we want it to grab multiple columns? Well, right now it doesn't look like we can grab two\"],\n"," [543.72,\n","  548.44,\n","  'columns using this method or this method. So in order to grab two columns, what we can do is a'],\n"," [548.44,\n","  552.7,\n","  \"similar method to what we use for our international plan, except this time we're going to pass through\"],\n"," [552.7,\n","  561.32,\n","  'an array of the columns that we want. And you can see by passing through an array within our'],\n"," [561.32,\n","  566.28,\n","  \"data frame sort of index, we're now able to grab both our state and our international plan.\"],\n"," [566.7,\n","  570.58,\n","  \"I should also note that I'm going to share this entire Jupyter Notebook with all the code\"],\n"," [570.58,\n","  575.78,\n","  \"completed inside of GitHub repository. It'll be in the descriptions below. I'd also love to know\"],\n"," [575.78,\n","  580.34,\n","  \"what you're using pandas for. Definitely leave a comment in the comments below. Alrighty, so back to\"],\n"," [580.34,\n","  584.94,\n","  \"our column. So we've now gone and shown our two different columns. Now again, whenever we're\"],\n"," [584.94,\n","  590.48,\n","  'performing our exploratory data analysis step using pandas, what we might like to do is find our'],\n"," [590.48,\n","  596.08,\n","  'unique values within each one of these columns. So what we can do is call the unique method to grab'],\n"," [596.08,\n","  602.34,\n","  \"our unique values. And just like that, by calling unique on our state column, we're now able to get\"],\n"," [602.34,\n","  608.2,\n","  'all the unique values within our data frame. Likewise, say we wanted to call it on our'],\n"," [610.34,\n","  617.1,\n","  \"churn column, for example, which we've got here. We can just change that state value to churn.\"],\n"," [617.56,\n","  622.64,\n","  \"And we're now able to see all the unique values within our churn column. So that covers filtering\"],\n"," [622.64,\n","  627.86,\n","  \"on column. So we've taken a look at how to grab a single column, how to grab a single column where\"],\n"," [627.86,\n","  632.54,\n","  \"there's a space in between it, as well as how to grab multiple columns and how to grab unique values.\"],\n"," [632.96,\n","  637.4,\n","  \"Now what happens if we wanted to filter on rows? Well, let's take a look at the rows that we've got.\"],\n"," [638.16,\n","  644.2,\n","  'So say we wanted to grab rows that had international plans set to know. Well, what we can do is'],\n"," [644.2,\n","  651.7,\n","  'use condition-race filtering to grab those values. So by passing through our data frame,'],\n"," [651.92,\n","  656.46,\n","  \"then selecting the column that we want to filter on, as well as the condition, we're able to grab\"],\n"," [656.46,\n","  661.0,\n","  'only the values that we want. So this is particularly useful when you want to filter your data frame'],\n"," [661.0,\n","  666.34,\n","  'or grab a subset of values. Now you can also filter on two different columns or have multiple'],\n"," [666.34,\n","  670.14,\n","  \"conditions. You just need to tweak this code a little bit. So let's copy this.\"],\n"," [671.8,\n","  677.72,\n","  'And say we wanted to take a look at values within our data set where customers churn, so customers'],\n"," [677.72,\n","  682.56,\n","  \"left our business, and they weren't on the international plans. International plans going to stay\"],\n"," [682.56,\n","  687.54,\n","  'as no. All we need to do to do that is pass through our different conditions inside of parentheses.'],\n"," [688.1,\n","  692.4,\n","  \"So let's put in our first condition inside of a set of parentheses, then pass through an\"],\n"," [692.4,\n","  696.66,\n","  \"ampersam to represent N. So we're basically concatenating all of these conditions together.\"],\n"," [697.08,\n","  701.4,\n","  \"And then we want to pass through our second condition, which is customers that didn't leave the\"],\n"," [701.4,\n","  705.34,\n","  \"business. So now what we're going to do is specify a different column. So in this case, we're\"],\n"," [705.34,\n","  712.46,\n","  \"filtering on churn and we want customers that didn't leave. So false. And now you can see that we're\"],\n"," [712.46,\n","  717.66,\n","  'actually filtering on both our international plan column as well as our churn column. If we'],\n"," [717.66,\n","  722.36,\n","  \"wanted to see customers that weren't on the international plan and did churn, all we need to do\"],\n"," [722.36,\n","  728.3,\n","  \"is change our second condition to true. And now you can see that we've been able to filter on rows.\"],\n"," [728.9,\n","  733.46,\n","  \"So what we've done is we've taken a look at our top five columns. We've filtered on rows where\"],\n"," [733.46,\n","  739.36,\n","  \"international plan equals no. And then we've also done a dual condition filter in order to pass\"],\n"," [739.36,\n","  744.04,\n","  'through additional conditions to that filter. Again, you can stack more of these together if you'],\n"," [744.04,\n","  749.34,\n","  \"wanted to build bigger filters. Now what we're going to take a look at is how we can index with\"],\n"," [749.34,\n","  755.64,\n","  'ILOC. So indexing allows you to filter through your data frame, but instead of using specific values'],\n"," [755.64,\n","  761.54,\n","  \"or specific conditions, we're going to be using integers. So say we wanted to grab our 15th row\"],\n"," [761.54,\n","  767.38,\n","  'within our data frame. Well, we can do this pretty easily using the ILOC method. And there you go.'],\n"," [767.54,\n","  772.6,\n","  \"We've now grabbed our 15th row. So what we've done there is we've taken our data frame and then we've\"],\n"," [772.6,\n","  777.52,\n","  \"used the ILOC method and then passed through 14. Now you're probably thinking why 14. Well,\"],\n"," [777.54,\n","  782.48,\n","  'remember at the start that I said that your data frame begins at row zero. So if we wanted our 15th'],\n"," [782.48,\n","  787.66,\n","  'row, we just need to subtract one. And this is going to give us a data frame. Now the ILOC function'],\n"," [787.66,\n","  791.7,\n","  'is really, really powerful. So it allows you to hone down and get the specific data that you want.'],\n"," [792.16,\n","  797.4,\n","  'To say, for example, you only wanted the state value as well. Well, we can extend our ILOC method'],\n"," [798.4,\n","  803.32,\n","  'and just pass through our first column. So when we pass through two different parameters, the first'],\n"," [803.32,\n","  808.46,\n","  'is going to be your row. The second is going to be your column. So by passing through column zero,'],\n"," [808.7,\n","  814.94,\n","  \"we're going to get our first column. And again, you can see we're now getting our state. Likewise,\"],\n"," [815.14,\n","  819.94,\n","  \"we can also change this and pass through negative one. And it's going to go the other way. So now we're\"],\n"," [819.94,\n","  825.28,\n","  'getting our last column, which is churn. Now we can also use the ILOC function to do slicing. So say'],\n"," [825.28,\n","  831.4,\n","  'we wanted a subset of our data frame. So rows 22 to 33, for example, well, what we can do is take'],\n"," [831.4,\n","  837.82,\n","  \"a slice of our data frame using the ILOC function. So we've used the ILOC method the same way,\"],\n"," [837.88,\n","  842.46,\n","  \"except this time, what we've done is we've passed through a segment of our data frame that we want.\"],\n"," [842.5,\n","  847.78,\n","  \"So we want to start at row 22 and grab everything to row 33, excluding it. So you can see that we've\"],\n"," [847.78,\n","  853.58,\n","  \"now gone and grab that slice. So that really covers how to work with ILOC. So it's really an\"],\n"," [853.58,\n","  859.96,\n","  'integer-based filtering function. Now we can also work with the lock methods. The lock method'],\n"," [859.96,\n","  865.1,\n","  'works a little bit differently to the ILOC method in that it uses keywords instead of integers.'],\n"," [865.72,\n","  870.74,\n","  'So in order to do that first, you need to have an index set on your data frame. So right now by'],\n"," [870.74,\n","  875.86,\n","  'default, when we read in our data frame, our index is just going to be created as a set of integers.'],\n"," [876.12,\n","  880.68,\n","  \"That's why we've got these numbers here. What we can do is actually set an index on a specific\"],\n"," [880.68,\n","  883.98,\n","  'column that we want. To say we wanted to set it on state, we can do that.'],\n"," [890.0,\n","  894.5,\n","  \"Alrighty. So you can see that we've now gone and set an index on state. So in order to do that,\"],\n"," [894.56,\n","  899.98,\n","  'we took a copy of our data frame. We then went and set our index and passed through the column'],\n"," [899.98,\n","  904.58,\n","  'that we want our index on. And we also passed through the in place parameter. So this is going to'],\n"," [904.58,\n","  909.6,\n","  \"allow it to do it in that data frame without having to create a copy of it. Then we've gone and\"],\n"," [909.6,\n","  915.14,\n","  'visualized those first-hop rows and you can see that state is indeed our index. Now what we can'],\n"," [915.14,\n","  920.24,\n","  'do is use the lock method to filter through our data frame and grab the specific rows that we want.'],\n"," [920.48,\n","  926.06,\n","  'So say for example, we only wanted rows which were in ILOC. Well, what we can do is use the'],\n"," [926.06,\n","  932.04,\n","  \"lock method to do that. So there you go. So we've just used the lock method to go and filter\"],\n"," [932.04,\n","  937.36,\n","  'through our data frame and just grab rows that are situated in ILOC. And that about wraps up our'],\n"," [937.36,\n","  944.86,\n","  \"read section. So we've now gone through how to show our top five and bottom five rows,\"],\n"," [945.42,\n","  952.68,\n","  \"show our columns and data types, create summary statistics, filter, filter on rows. And we've also\"],\n"," [952.68,\n","  956.96,\n","  \"done some indexing with ILOC and lock. Now it's time to get to updating. So the first thing that\"],\n"," [956.96,\n","  962.16,\n","  \"we're going to take a look at is how to drop rows. So whenever you're pre-processing your data frame,\"],\n"," [962.3,\n","  966.42,\n","  \"one of the steps that you're probably going to want to do is drop values that have nulls.\"],\n"," [966.42,\n","  970.68,\n","  'Now what we can do is actually calculate the rows that have nulls using the is null method.'],\n"," [971.8,\n","  977.18,\n","  \"So by training together is null and sum, we're able to see the number of rows that have missing\"],\n"," [977.18,\n","  981.38,\n","  \"values. In this case, it's represented by the different columns. So it looks like we've got 10 rows\"],\n"," [981.38,\n","  988.04,\n","  'missing total day minutes, 10 missing total day calls, 8 missing churn, a whole bunch of others'],\n"," [988.04,\n","  993.08,\n","  \"there. Now with pandas, we represent these as NA's. So what we can actually do in order to drop\"],\n"," [993.08,\n","  1000.36,\n","  'these rows is use the drop NA function. So by using drop NA and then passing through in place'],\n"," [1000.36,\n","  1005.02,\n","  \"equals true, we're able to drop all the rows that have missing values. So if we go and visualize\"],\n"," [1005.02,\n","  1010.9,\n","  'this again, you can see that we now have zero missing values within our data frame. So what we did'],\n"," [1010.9,\n","  1015.46,\n","  \"there is we just visualize the number of rows that have missing values and then we've gone and dropped\"],\n"," [1015.46,\n","  1020.32,\n","  \"all of those values. Now what we can also do is we can drop columns. So say we didn't actually\"],\n"," [1020.32,\n","  1024.62,\n","  'want a column. For example, say we wanted to drop area code, for example, what we can also do'],\n"," [1024.62,\n","  1033.92,\n","  \"is drop a column using the drop method. So let's do that. So simply by using the df.drop method,\"],\n"," [1033.92,\n","  1039.2,\n","  \"we're able to drop a column. So in this case, what we've done is we've dropped the area code column\"],\n"," [1039.2,\n","  1043.62,\n","  \"and we've also passed through access equals one. Because in this case, we're dropping a column,\"],\n"," [1043.76,\n","  1048.38,\n","  \"we're not dropping a row. What about creating columns? So say, for example, we wanted to create a column\"],\n"," [1048.38,\n","  1054.16,\n","  'which was the sum of total night minutes and total international minutes. So what we can do is'],\n"," [1054.16,\n","  1064.46,\n","  \"really quickly add a new column and add those together. And there you go. That's how you create a\"],\n"," [1064.46,\n","  1069.8,\n","  'new calculator column. So what we basically did is we grabbed our total night minutes plus our total'],\n"," [1069.8,\n","  1074.46,\n","  \"international minutes and then we've added those together and we've put it in a new column\"],\n"," [1074.46,\n","  1080.34,\n","  'called new column. So if we go and take a look at these now, you can see that our new column is'],\n"," [1080.34,\n","  1086.52,\n","  \"there on the far right hand side. So what we can also do is say we weren't happy with the values\"],\n"," [1086.52,\n","  1090.76,\n","  'in this. Well, what we can do is update that entire column. So say we wanted everything in here'],\n"," [1090.76,\n","  1098.26,\n","  'to read 100. Well, we can grab that column and set it equal to 100. Now if we go and visualize it,'],\n"," [1098.26,\n","  1104.54,\n","  \"everything within that column is reading up as 100. So that's gone and updated everything in\"],\n"," [1104.54,\n","  1108.96,\n","  'that column. What happens if we just wanted to update a single value? For example, our first value'],\n"," [1108.96,\n","  1114.14,\n","  \"here. Well, this is where we can use ILOC to go and replace those values. So let's go and grab our\"],\n"," [1114.14,\n","  1122.76,\n","  \"first column and then set it to 10. Now if we go and visualize that column, you can see that it's\"],\n"," [1122.76,\n","  1128.1,\n","  'now reading as 10. So what we did is we first grabbed the value using ILOC. So we grabbed as'],\n"," [1128.1,\n","  1133.12,\n","  'first row, which is zero and then we grabbed our last column, which we can access by passing through'],\n"," [1133.12,\n","  1138.62,\n","  \"negative one and then we set that value to 10. So you can see that that's now reading as 10.\"],\n"," [1138.94,\n","  1143.56,\n","  'Now the last update methodology that I want to walk you through is how to use the apply function.'],\n"," [1143.62,\n","  1148.96,\n","  'So the apply function is super powerful because you can basically iterate through every value within'],\n"," [1148.96,\n","  1153.82,\n","  'a specific column or within your data frame and make updates appropriately using conditions.'],\n"," [1154.34,\n","  1159.36,\n","  'So say we wanted to take our churn column and rather than it being represented as falses and'],\n"," [1159.36,\n","  1166.2,\n","  'trues, we wanted it represented as ones and zeros in a new column called churn binary. Well, because'],\n"," [1166.2,\n","  1170.46,\n","  \"there's different conditions there, what we need to do is have a little bit of filtering in order\"],\n"," [1170.46,\n","  1174.52,\n","  'to determine how to update that value. This is where the apply method comes in.'],\n"," [1180.26,\n","  1185.94,\n","  \"And there you go. So we've gone and used the apply method on our churn column and then we've\"],\n"," [1185.94,\n","  1191.6,\n","  \"used lambda to basically iterate through each one of these values and we've also used a expression\"],\n"," [1191.6,\n","  1196.52,\n","  \"here. So we basically said it's very pythonic. So we've basically said if x equals true,\"],\n"," [1196.88,\n","  1200.56,\n","  'set it to one else, set it to zero. So if we go and take a look at our columns now,'],\n"," [1201.96,\n","  1207.78,\n","  \"you can see that we now have a churn binary. So let's double check that we've done that for our\"],\n"," [1207.78,\n","  1215.58,\n","  'true values as well. And you can see that if churn equals true, our churn binary equals one.'],\n"," [1216.02,\n","  1220.02,\n","  'So applies really, really powerful because you can even pass through functions in here and make'],\n"," [1220.02,\n","  1224.8,\n","  \"updates that way. Just to keep point to note out, x is going to represent the value that you're\"],\n"," [1224.8,\n","  1228.44,\n","  \"iterating through at a point in time. That's what we've said x equals true.\"],\n"," [1229.64,\n","  1234.6,\n","  'All right. And that about wraps up our updating section. So what we did is we dropped some rows'],\n"," [1234.6,\n","  1240.68,\n","  \"and we specifically dropped our n a's. We've also dropped some columns. We've created new calculated\"],\n"," [1240.68,\n","  1245.8,\n","  'columns. So specifically we added our night minutes plus our international minutes to create our'],\n"," [1245.8,\n","  1251.4,\n","  'new column here. We took a look at how to update an entire column, how to update a single value,'],\n"," [1251.6,\n","  1256.38,\n","  'and last but not least we took a look at how we can use the apply method to provide a conditional'],\n"," [1256.38,\n","  1261.44,\n","  \"based updates. The last thing that we're going to do is take a look at our delete and output\"],\n"," [1261.44,\n","  1266.56,\n","  'section. So say we wanted to output our data frame to a CSV, output it to Jason or output it to'],\n"," [1266.56,\n","  1270.9,\n","  \"HTML. This is where we're going to do it. So the first thing that we're going to take a look at is\"],\n"," [1270.9,\n","  1276.7,\n","  'how to output our data frame to a CSV. This is pretty easy. All we need to do is type in df.2 CSV'],\n"," [1278.1200000000001,\n","  1282.14,\n","  \"and then name it. So what we'll do is we'll just call it output for now. .csv.\"],\n"," [1282.94,\n","  1287.84,\n","  'This is going to output in the same folder that our Jupyter Notebook is in. So you can see if we'],\n"," [1287.84,\n","  1296.42,\n","  \"scroll over here, we've now got an output.csv data set. So you can see that we've got our columns that\"],\n"," [1296.42,\n","  1301.88,\n","  'we made updates to. Those are all represented within that data frame. Now likewise we can also'],\n"," [1301.88,\n","  1305.76,\n","  \"output to Jason. For example, if you're working with different web components, working with Jason\"],\n"," [1305.76,\n","  1310.9,\n","  \"is obviously super useful. That's pretty easy. All we need to do is type in two Jason.\"],\n"," [1311.7,\n","  1317.56,\n","  \"And it's going to create all of the Jason for our data frame. Likewise doing it for HTML.\"],\n"," [1317.84,\n","  1325.32,\n","  \"Sort of similar method. And now we've got a HTML representation of our data frame. So again,\"],\n"," [1325.42,\n","  1331.78,\n","  \"we've just passed through two HTML to get the HTML, two Jason to get the Jason. And if we wanted to\"],\n"," [1331.78,\n","  1337.6,\n","  'delete our data frame, all we need to do is type in df. So this is a Python method that allows us to'],\n"," [1337.6,\n","  1343.04,\n","  'get rid of a specific object. And that about wraps up our last section, which is outputting and'],\n"," [1343.04,\n","  1350.4,\n","  'deletion. So we outputted to CSV to Jason to HTML. And last but not least, we deleted our data frame.'],\n"," [1350.52,\n","  1354.7,\n","  'And that about wraps up this Python crash course. So what we did is we went through how to create'],\n"," [1354.7,\n","  1360.04,\n","  'data frames, how to make updates to them, how to read them and change data. And last but not least,'],\n"," [1360.1,\n","  1364.54,\n","  'we also took a look at how we can output our data and delete it. Thanks so much for tuning in,'],\n"," [1364.54,\n","  1368.04,\n","  'guys. Hopefully you found this video useful. If you did, be sure to hit subscribe,'],\n"," [1368.04,\n","  1372.56,\n","  \"tick that bell so you get notified of any future videos that I release. And if you've got\"],\n"," [1372.56,\n","  1377.36,\n","  'any questions at all, I mean anything, be sure to drop and mention in the comments below and'],\n"," [1377.36,\n","  1379.94,\n","  \"I'll get right back to you. Thanks again for tuning in. Peace.\"]]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11074,"status":"ok","timestamp":1730354103894,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"jK2P66C0I-j0","outputId":"3ba0560b-5f49-4582-aeb8-4132e3d2d617"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting faster-whisper\n","  Downloading faster_whisper-1.0.3-py3-none-any.whl.metadata (15 kB)\n","Collecting av<13,>=11.0 (from faster-whisper)\n","  Downloading av-12.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n","Collecting ctranslate2<5,>=4.0 (from faster-whisper)\n","  Downloading ctranslate2-4.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.10/dist-packages (from faster-whisper) (0.24.7)\n","Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.10/dist-packages (from faster-whisper) (0.19.1)\n","Collecting onnxruntime<2,>=1.14 (from faster-whisper)\n","  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (75.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (1.26.4)\n","Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2024.6.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (24.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.66.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.12.2)\n","Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (24.3.25)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.13.1)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2024.8.30)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n","Downloading faster_whisper-1.0.3-py3-none-any.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading av-12.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ctranslate2-4.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m870.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: humanfriendly, ctranslate2, av, coloredlogs, onnxruntime, faster-whisper\n","Successfully installed av-12.3.0 coloredlogs-15.0.1 ctranslate2-4.5.0 faster-whisper-1.0.3 humanfriendly-10.0 onnxruntime-1.19.2\n"]}],"source":["!pip install faster-whisper"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yf37guo4JlG9"},"outputs":[],"source":["from google.colab import auth\n","auth.authenticate_user()\n","\n","import os\n","os.environ[\"HF_TOKEN\"] = \"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\"  # Replace with your actual token\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bp6fnrZeKPHs","executionInfo":{"status":"ok","timestamp":1730355230259,"user_tz":-330,"elapsed":13261,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"}},"outputId":"edccffcc-def4-4857-8c13-9167b621b5a2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["from faster_whisper import WhisperModel\n","model = WhisperModel(\"base\")"]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jEVGxE0uNYZN","executionInfo":{"status":"ok","timestamp":1730355254759,"user_tz":-330,"elapsed":11086,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"}},"outputId":"bbda377a-039a-4d54-afb2-a786de1e77f7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<faster_whisper.transcribe.WhisperModel at 0x7ce2dddcd930>"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dOxUWxZFI5l2","outputId":"9f5dab37-d33b-4696-db99-5064d0c9e1a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Transcribing chunk_1.mp3...\n"]}],"source":["from faster_whisper import WhisperModel\n","import os\n","\n","def transcribe_audio_chunks(audio_folder,model, model_name=\"small\"):\n","    # Check if CUDA is available and set the device accordingly\n","\n","    # Initialize the Faster Whisper model\n","      # Use \"cuda\" if available, else fall back to \"cpu\"\n","\n","    # List all audio files in the folder\n","    audio_files = sorted([f for f in os.listdir(audio_folder) if f.endswith('.mp3')])\n","\n","    # Store transcriptions with timestamps\n","    all_transcriptions = []\n","\n","    for i, audio_file in enumerate(audio_files, start=1):\n","        audio_path = os.path.join(audio_folder, f\"chunk_{i}.mp3\")\n","        print(f\"Transcribing {audio_file}...\")\n","\n","        # Transcribe audio with timestamps\n","        segments, _ = model.transcribe(audio_path, language=\"en\")\n","\n","        # Extract text and timestamps for each segment\n","        transcription = {\n","            \"chunk\": i,\n","            \"transcription\": [\n","                {\"start\": segment.start, \"end\": segment.end, \"text\": segment.text}\n","                for segment in segments\n","            ]\n","        }\n","\n","        all_transcriptions.append(transcription)\n","\n","        # Print the transcription with timestamps\n","        print(f\"Chunk {i} Transcription:\")\n","        for segment in transcription[\"transcription\"]:\n","            print(f\"[{segment['start']} - {segment['end']}] {segment['text']}\")\n","\n","    return all_transcriptions\n","transcriptions = transcribe_audio_chunks(mainPath,model, model_name=\"base\")"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"executionInfo":{"elapsed":786,"status":"error","timestamp":1730431993995,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"w-cf7xcNjDKm","outputId":"5eb4ca0a-f3cd-4820-d636-9ce8663e152b"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"load_model() got multiple values for argument 'device'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-8d311ff14c72>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msmall_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"base\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: load_model() got multiple values for argument 'device'"]}],"source":["import whisper\n","\n","small_model = whisper.load_model(\"base\")\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20177,"status":"ok","timestamp":1730432064381,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"_vV3bZn0_L3q","outputId":"11919a1f-0a2b-4591-df93-d71bd57b1599"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.000s --> 7.620s] Which of these is the most powerful AI company? Is it A Open AI, D Google, C Meta or D None of\n","[7.620s --> 12.520s] these? Many people argue that it's none of these. All of these companies have done a great job\n","[12.520s --> 18.120s] at building foundational AI models. But they rely on Nvidia's GPUs to train them. None of this\n","[18.120s --> 23.100s] representative AI magic would be possible if Nvidia did not exist. Today I want to show you how\n","[23.100s --> 27.580s] you can learn AI from Nvidia and get certified by them. Let's do this.\n","[28.720s --> 34.120s] Nvidia offers many courses to learn the essentials of AI. Most of these courses are free,\n","[34.420s --> 38.780s] but there are some that are paid as well. Let's start with some free courses. Let's cover\n","[38.780s --> 43.300s] the basics first. The first course I want to talk about requires no programming knowledge.\n","[43.680s --> 48.780s] The course is called Generative AI Explained. Generative AI is a type of artificial intelligence\n","[48.780s --> 54.640s] that can create new content like images, text or music. Think of tools like Dali creating images\n","[54.640s --> 60.080s] from text descriptions or chat GPT generating human like responses. The course will define\n","[60.080s --> 64.520s] Generative AI and explain how it works. It will show you all the different applications of\n","[64.520s --> 68.940s] Generative AI and tell you about the potential opportunities in the field. You will learn about\n","[68.940s --> 73.460s] the underlying technologies behind Generative AI such as neural networks and deep learning.\n","[73.760s --> 77.740s] By the end of this course, you will have a good understanding of what Generative AI is capable\n","[77.740s --> 84.300s] of and how it is shaping different industries. Moving on. If you have ever worked with GPT or\n","[84.300s --> 89.320s] any other similar large language model, you would know that sometimes it can just make stuff up.\n","[89.640s --> 94.320s] This is also called hallucinations. For this reason, most people still use Google to find\n","[94.320s --> 98.460s] information that is more factually correct. But if you are following this space closely,\n","[98.720s --> 103.120s] you would know that there are startups like perplexity that provide chat GPT like text generation,\n","[103.560s --> 108.620s] but are factually accurate. So how do they do it? They use something called Retrieval Augmented\n","[108.620s --> 114.160s] Generation or Rag. Let's break down what Retrieval Augmented Generation means. Retrieval simply\n","[114.160s --> 119.080s] means finding relevant information from a knowledge base. This could be a database of facts,\n","[119.400s --> 125.680s] articles or search results. Augmented means improving something. In this context, it means improving\n","[125.680s --> 130.640s] AI's knowledge with the help of retrieved information. Generation refers to creating new content\n","[130.640s --> 136.040s] which is what the large language model does. In simple words, with Rag, we retrieve the relevant\n","[136.040s --> 141.460s] information and use it to augment AI's knowledge before generating a response. It's an end to\n","[141.460s --> 145.780s] a architecture that combines the information retrieval component with a response generator.\n","[146.240s --> 151.620s] This approach helps to ground the AI's responses in factual information, reducing the likelihood of\n","[151.620s --> 156.700s] hallucinations. If you want to learn more about how to use Rag with LLMs, you can do this course\n","[156.700s --> 162.180s] called Augmented or LLM using the Treeval Augmented Generation by Nvidia. This course will cover\n","[162.180s --> 167.760s] the Rag components that Nvidia uses internally. It's a great way to start your journey with LLMs and\n","[167.760s --> 174.920s] Rag. In the beginning of this video, we talked about how most companies are using Nvidia CPUs to\n","[174.920s --> 181.380s] train models. But what makes Nvidia CPU so special? It's their parallel processing power. Nvidia\n","[181.380s --> 187.120s] CPUs contain thousands of smaller cores that can perform tasks simultaneously. This makes them a\n","[187.120s --> 192.220s] go-to choice for AI model training where parallel processing is critical. Let's understand why\n","[192.220s --> 197.180s] that is. Consider the task of training a large language model. These models usually have\n","[197.180s --> 202.120s] billions of parameters that need to be adjusted during the training. If you were to do it sequentially,\n","[202.580s --> 207.540s] one calculation at a time, it would take an impractically long amount of time to train. But with\n","[207.540s --> 213.360s] parallel processing, you can perform these calculations simultaneously. It significantly reduces the time\n","[213.360s --> 219.180s] to train. Compute unified device architecture or CUDA allows developers to tap into GPUs parallel\n","[219.180s --> 224.680s] processing power. In simple words, CUDA makes it easy to write code that can be run in parallel\n","[224.680s --> 229.900s] on a GPU. If you want to learn how to write parallel CUDA kernels, you can do this course called\n","[229.900s --> 235.100s] an even easier introduction to CUDA. This course will teach you how to organize parallel thread\n","[235.100s --> 240.140s] execution and efficiently manage memory. If you have played around with chat,\n","[240.140s --> 245.000s] activity or Claude, you would know that the response you get from them is as good as the questions\n","[245.000s --> 250.120s] you ask them. The art of crafting effective prompts is becoming increasingly important as AI\n","[250.120s --> 255.160s] models become more sophisticated and widely used. Asking the right questions or prompt engineering\n","[255.160s --> 259.380s] is going to become a highly valuable skill in the future. To learn prompt engineering,\n","[259.780s --> 265.200s] you can do this course called prompt engineering with Lama 2. For those who don't know, Lama is the\n","[265.200s --> 270.260s] open source slash language model from Meta. This course will teach you how to write precise prompts\n","[270.260s --> 275.020s] to achieve the desired LLM behavior. You will use techniques like few short learning where you\n","[275.020s --> 281.300s] provide examples in your prompt to guide models output. We have talked a lot about generative AI,\n","[281.720s --> 287.440s] LLMs and chat GPD so far. All these are applications of something called deep learning. Deep learning\n","[287.440s --> 292.080s] is a subset of machine learning techniques where we use artificial neural networks to learn from\n","[292.080s --> 297.380s] large amounts of data. You can get started with deep learning by using this course. All the courses\n","[297.380s --> 301.780s] that we have talked about before this one are less than 3 hours and are more high level. This\n","[301.780s --> 306.720s] course is much more hands on. This course goes a little deeper. You would need to know Python if\n","[306.720s --> 311.140s] you want to do this course. This course will teach you PyTorch which is a popular deep learning\n","[311.140s --> 317.220s] library. You will also learn convolutional neural networks or CNNs. CNNs are particularly effective for\n","[317.220s --> 322.360s] processing grid-like data like images. This course also covers transfer learning and natural\n","[322.360s --> 329.080s] language processing. All the courses we discussed can be done separately but if you like slightly\n","[329.080s --> 334.400s] more structured approach Nvidia also offers some learning paths. They have a foundational learning\n","[334.400s --> 339.660s] path for basics. This path is for those who are new to AI and want to build a solid foundation.\n","[340.000s --> 345.280s] They also have a path for generative AI and LLMs. This path goes deeper into the technology's\n","[345.280s --> 350.360s] powering latest AI breakthroughs. It covers topics like transformer architecture and diffusion\n","[350.360s --> 357.520s] models. So far we have only talked about how to learn new AI skills but what if you want to get\n","[357.520s --> 363.340s] certified for your skills. Nvidia also offers certification for different skills. These certifications\n","[363.340s --> 367.920s] can be a great way to validate your skills and stand out in the job market. For example,\n","[368.200s --> 373.340s] you can become Nvidia certified for generative AI LLMs after doing some of the courses we discussed\n","[373.340s --> 378.500s] today. They also offer some other certifications like AI Infra and Operations Certificate and\n","[378.500s --> 383.580s] Multi-Model Genie Certificate. The AI Infra and Operations Certificate can be valuable if you're\n","[383.580s --> 389.480s] interested in DevOps side of AI. The Multi-Model Genie Certification covers systems that can work with\n","[389.480s --> 394.860s] multiple types of data such as text, images and audio. Whether you are just starting out your AI\n","[394.860s --> 399.840s] journey or trying to specialize in a particular area, these courses and certifications will work as\n","[399.840s --> 404.520s] a great resource for you. I will leave a link to all of them down below. If you want to know the\n","[404.520s --> 410.320s] fastest way to learn programming using AI, watch this video. My name is Sahil and I'll see you in the\n","[410.320s --> 410.800s] next one.\n"]}],"source":["result = model.transcribe(mainPath+'/data/Youtube_video_audio.wav',word_timestamps=True)\n","original_transcript=[]\n","# Extract and print sentence-level timestamps and transcription\n","for segment in result['segments']:\n","    stmt=[segment['start'],segment['end'],segment['text'].strip()]\n","    original_transcript.append(stmt)\n","    start = segment['start']\n","    end = segment['end']\n","    text = segment['text'].strip()  # Clean up any extra whitespace\n","    print(f\"[{start:.3f}s --> {end:.3f}s] {text}\")"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":524,"status":"ok","timestamp":1730432077701,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"UQY9oVzahtpW","outputId":"77941944-3c8f-4395-96ce-42752f2a1e2b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0.0,\n","  7.62,\n","  'Which of these is the most powerful AI company? Is it A Open AI, D Google, C Meta or D None of'],\n"," [7.62,\n","  12.52,\n","  \"these? Many people argue that it's none of these. All of these companies have done a great job\"],\n"," [12.52,\n","  18.12,\n","  \"at building foundational AI models. But they rely on Nvidia's GPUs to train them. None of this\"],\n"," [18.12,\n","  23.1,\n","  'representative AI magic would be possible if Nvidia did not exist. Today I want to show you how'],\n"," [23.1,\n","  27.58,\n","  \"you can learn AI from Nvidia and get certified by them. Let's do this.\"],\n"," [28.719999999999995,\n","  34.12,\n","  'Nvidia offers many courses to learn the essentials of AI. Most of these courses are free,'],\n"," [34.42,\n","  38.78,\n","  \"but there are some that are paid as well. Let's start with some free courses. Let's cover\"],\n"," [38.78,\n","  43.3,\n","  'the basics first. The first course I want to talk about requires no programming knowledge.'],\n"," [43.68,\n","  48.78,\n","  'The course is called Generative AI Explained. Generative AI is a type of artificial intelligence'],\n"," [48.78,\n","  54.64,\n","  'that can create new content like images, text or music. Think of tools like Dali creating images'],\n"," [54.64,\n","  60.08,\n","  'from text descriptions or chat GPT generating human like responses. The course will define'],\n"," [60.08,\n","  64.52,\n","  'Generative AI and explain how it works. It will show you all the different applications of'],\n"," [64.52,\n","  68.94,\n","  'Generative AI and tell you about the potential opportunities in the field. You will learn about'],\n"," [68.94,\n","  73.46,\n","  'the underlying technologies behind Generative AI such as neural networks and deep learning.'],\n"," [73.76,\n","  77.74,\n","  'By the end of this course, you will have a good understanding of what Generative AI is capable'],\n"," [77.74,\n","  84.3,\n","  'of and how it is shaping different industries. Moving on. If you have ever worked with GPT or'],\n"," [84.3,\n","  89.32,\n","  'any other similar large language model, you would know that sometimes it can just make stuff up.'],\n"," [89.64,\n","  94.32,\n","  'This is also called hallucinations. For this reason, most people still use Google to find'],\n"," [94.32,\n","  98.46,\n","  'information that is more factually correct. But if you are following this space closely,'],\n"," [98.72,\n","  103.12,\n","  'you would know that there are startups like perplexity that provide chat GPT like text generation,'],\n"," [103.56,\n","  108.62,\n","  'but are factually accurate. So how do they do it? They use something called Retrieval Augmented'],\n"," [108.62,\n","  114.16,\n","  \"Generation or Rag. Let's break down what Retrieval Augmented Generation means. Retrieval simply\"],\n"," [114.16,\n","  119.08,\n","  'means finding relevant information from a knowledge base. This could be a database of facts,'],\n"," [119.4,\n","  125.68,\n","  'articles or search results. Augmented means improving something. In this context, it means improving'],\n"," [125.68,\n","  130.64,\n","  \"AI's knowledge with the help of retrieved information. Generation refers to creating new content\"],\n"," [130.64,\n","  136.04,\n","  'which is what the large language model does. In simple words, with Rag, we retrieve the relevant'],\n"," [136.04,\n","  141.46,\n","  \"information and use it to augment AI's knowledge before generating a response. It's an end to\"],\n"," [141.46,\n","  145.78,\n","  'a architecture that combines the information retrieval component with a response generator.'],\n"," [146.24,\n","  151.62,\n","  \"This approach helps to ground the AI's responses in factual information, reducing the likelihood of\"],\n"," [151.62,\n","  156.7,\n","  'hallucinations. If you want to learn more about how to use Rag with LLMs, you can do this course'],\n"," [156.7,\n","  162.18,\n","  'called Augmented or LLM using the Treeval Augmented Generation by Nvidia. This course will cover'],\n"," [162.18,\n","  167.76,\n","  \"the Rag components that Nvidia uses internally. It's a great way to start your journey with LLMs and\"],\n"," [167.76,\n","  174.92,\n","  'Rag. In the beginning of this video, we talked about how most companies are using Nvidia CPUs to'],\n"," [174.92,\n","  181.38,\n","  \"train models. But what makes Nvidia CPU so special? It's their parallel processing power. Nvidia\"],\n"," [181.38,\n","  187.12,\n","  'CPUs contain thousands of smaller cores that can perform tasks simultaneously. This makes them a'],\n"," [187.12,\n","  192.22,\n","  \"go-to choice for AI model training where parallel processing is critical. Let's understand why\"],\n"," [192.22,\n","  197.18,\n","  'that is. Consider the task of training a large language model. These models usually have'],\n"," [197.18,\n","  202.12,\n","  'billions of parameters that need to be adjusted during the training. If you were to do it sequentially,'],\n"," [202.58,\n","  207.54,\n","  'one calculation at a time, it would take an impractically long amount of time to train. But with'],\n"," [207.54,\n","  213.36,\n","  'parallel processing, you can perform these calculations simultaneously. It significantly reduces the time'],\n"," [213.36,\n","  219.18,\n","  'to train. Compute unified device architecture or CUDA allows developers to tap into GPUs parallel'],\n"," [219.18,\n","  224.68,\n","  'processing power. In simple words, CUDA makes it easy to write code that can be run in parallel'],\n"," [224.68,\n","  229.9,\n","  'on a GPU. If you want to learn how to write parallel CUDA kernels, you can do this course called'],\n"," [229.9,\n","  235.1,\n","  'an even easier introduction to CUDA. This course will teach you how to organize parallel thread'],\n"," [235.1,\n","  240.14,\n","  'execution and efficiently manage memory. If you have played around with chat,'],\n"," [240.14,\n","  245.0,\n","  'activity or Claude, you would know that the response you get from them is as good as the questions'],\n"," [245.0,\n","  250.12,\n","  'you ask them. The art of crafting effective prompts is becoming increasingly important as AI'],\n"," [250.12,\n","  255.16,\n","  'models become more sophisticated and widely used. Asking the right questions or prompt engineering'],\n"," [255.16,\n","  259.38,\n","  'is going to become a highly valuable skill in the future. To learn prompt engineering,'],\n"," [259.78,\n","  265.2,\n","  \"you can do this course called prompt engineering with Lama 2. For those who don't know, Lama is the\"],\n"," [265.2,\n","  270.26,\n","  'open source slash language model from Meta. This course will teach you how to write precise prompts'],\n"," [270.26,\n","  275.02,\n","  'to achieve the desired LLM behavior. You will use techniques like few short learning where you'],\n"," [275.02,\n","  281.3,\n","  'provide examples in your prompt to guide models output. We have talked a lot about generative AI,'],\n"," [281.72,\n","  287.44,\n","  'LLMs and chat GPD so far. All these are applications of something called deep learning. Deep learning'],\n"," [287.44,\n","  292.08,\n","  'is a subset of machine learning techniques where we use artificial neural networks to learn from'],\n"," [292.08,\n","  297.38,\n","  'large amounts of data. You can get started with deep learning by using this course. All the courses'],\n"," [297.38,\n","  301.78,\n","  'that we have talked about before this one are less than 3 hours and are more high level. This'],\n"," [301.78,\n","  306.72,\n","  'course is much more hands on. This course goes a little deeper. You would need to know Python if'],\n"," [306.72,\n","  311.14,\n","  'you want to do this course. This course will teach you PyTorch which is a popular deep learning'],\n"," [311.14,\n","  317.22,\n","  'library. You will also learn convolutional neural networks or CNNs. CNNs are particularly effective for'],\n"," [317.22,\n","  322.36,\n","  'processing grid-like data like images. This course also covers transfer learning and natural'],\n"," [322.36,\n","  329.08,\n","  'language processing. All the courses we discussed can be done separately but if you like slightly'],\n"," [329.08,\n","  334.4,\n","  'more structured approach Nvidia also offers some learning paths. They have a foundational learning'],\n"," [334.4,\n","  339.66,\n","  'path for basics. This path is for those who are new to AI and want to build a solid foundation.'],\n"," [340.0,\n","  345.28,\n","  \"They also have a path for generative AI and LLMs. This path goes deeper into the technology's\"],\n"," [345.28,\n","  350.36,\n","  'powering latest AI breakthroughs. It covers topics like transformer architecture and diffusion'],\n"," [350.36,\n","  357.52,\n","  'models. So far we have only talked about how to learn new AI skills but what if you want to get'],\n"," [357.52,\n","  363.34,\n","  'certified for your skills. Nvidia also offers certification for different skills. These certifications'],\n"," [363.34,\n","  367.92,\n","  'can be a great way to validate your skills and stand out in the job market. For example,'],\n"," [368.2,\n","  373.34,\n","  'you can become Nvidia certified for generative AI LLMs after doing some of the courses we discussed'],\n"," [373.34,\n","  378.5,\n","  'today. They also offer some other certifications like AI Infra and Operations Certificate and'],\n"," [378.5,\n","  383.58,\n","  \"Multi-Model Genie Certificate. The AI Infra and Operations Certificate can be valuable if you're\"],\n"," [383.58,\n","  389.48,\n","  'interested in DevOps side of AI. The Multi-Model Genie Certification covers systems that can work with'],\n"," [389.48,\n","  394.86,\n","  'multiple types of data such as text, images and audio. Whether you are just starting out your AI'],\n"," [394.86,\n","  399.84,\n","  'journey or trying to specialize in a particular area, these courses and certifications will work as'],\n"," [399.84,\n","  404.52,\n","  'a great resource for you. I will leave a link to all of them down below. If you want to know the'],\n"," [404.52,\n","  410.32,\n","  \"fastest way to learn programming using AI, watch this video. My name is Sahil and I'll see you in the\"],\n"," [410.32, 410.8, 'next one.']]"]},"metadata":{},"execution_count":15}],"source":["original_transcript"]},{"cell_type":"markdown","metadata":{"id":"W68ll-kSAqaf"},"source":["## Video segmenataion"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5543,"status":"ok","timestamp":1730294451675,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"Hq8jXakwAxZ-","outputId":"d9cfcf96-6b1a-4dfa-a96c-05a44f319c81"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting keybert\n","  Downloading keybert-0.8.5-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.26.4)\n","Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.9.3)\n","Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.5.2)\n","Collecting sentence-transformers>=0.3.8 (from keybert)\n","  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.18.0)\n","Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (4.12.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.44.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.66.5)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.5.0+cu121)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.24.7)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (10.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.6.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.9.11)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.19.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.8.30)\n","Downloading keybert-0.8.5-py3-none-any.whl (37 kB)\n","Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentence-transformers, keybert\n","Successfully installed keybert-0.8.5 sentence-transformers-3.2.1\n"]}],"source":["!pip install keybert"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":545,"referenced_widgets":["127984ef3f3e4827ab3b754273f028a6","30d30b86d98d443d8cdfddaa20a1edfe","64050d74a49f49cdb5a7e89906bd93d0","1977a2744a294877a4474b8288e56285","724d95e47dcb4511b7aea7f5676c2e45","aa9499cb907348649972ec22e53186f0","14d305f2a393475f84f8f8da471fe909","5cc23a37be544b21ad7a09529244ec9e","23aba67299ab4292aed339a15eeb53c1","837ef3cb84b3439390b214b59848b670","59ea89db489f4ad5b940fa0eedde7eb2","17eb7ba55f844e9ea34c3ed5af7fc4e2","567a69c762354c0689ec7b21df0614ad","aa6dbe0fb9814820874e6627080b407a","d452a3d313004c3b82bde3b8b0a3124d","15f4d1188627496a81792d03a95183bf","09803a494e4642509c2a4218219c71ec","d574387da41b4d388204b523671d4067","8059b3d85e6242c5a18f66c5bdc6a6a2","aab5e2ac56bc434eba045fb8295360d1","a60eb58dc44d4d8fab3c52ed3a4f2412","d72c852353d64bccb4e5e840b5f35f1e","6fb60459d45e4a28aa9347fd565aa58f","8112f9eccf9a44e0b437bcba1d0409ee","447b2ad1dc434914a623f4660d470817","b3c3c368e00d456c9fc5ea92f9d9841f","b13357615f5c4505b1825cc3263d8371","e5d214bc49884d18abbe014f42e680a4","4aecae8bfe6a4c168b5e93e453c500a5","758e6b5fa89c4e408e5a846fdc7e85eb","105a30493f56442eb2075f5a88d3c236","642fdc2f308549c8a5276993f7ef71e0","13771a3cbabc4e7d9fd707ce3d43f374","04f6c584044a4ccbb52714a22db2f925","dca31c359b99468fa78672b20788fb72","cc2243402ff0428291c9af8679a8c745","0c4758ebafc14e97876084f64e582a1d","48cfdae9c29242b1ac7b2cec01176832","1584976b17864a84aa1369f6e09a0ecd","63326948732a411a881eff2fa4890743","c4dd5e6919fe442b931ee61db75fdf95","e6905db330b8446b9ef2a3f53da2e222","5a48b47e527b443882b6dd2f608a35eb","6abf15c7d58949348593846555ae1e15","79cbc512027e4a36afa452baf7535c87","bbdb56fa6f3143f69b49641df075ac7b","cb44812f719e4e7da154549874969e9f","b00f7d8f1f954f6ebf4cc810a0ab9d08","5cb9b125f1b544e78eb7c5e9e6459b6f","1ed91e4f349d4f5f942df73fd74da144","0d866581898445c29069342b81af9f59","56af526d68a6441aa5e229edf1e711a0","ae7ed5c0f46a417e82089f1e6c02d824","2315022199bd45f096b706fc77d845ef","5c1ee2da4423411c819bbf593e899455","43cf306d7ba3439f8179c25a6538ca2f","c2644d5aa9904ff1bad6d918313378a5","d86544b996154fb6a930fc95d1ce41bb","0d39af349a0a46dbb3fa7c02fb860732","e4bcc6c7103a40adb9e60d7a19152da0","89d509e8fbef4d0884ab896f8f81f6e8","9aaebf0580754ecab53c6c9580a5dd7a","01c1a10e09c242ca8dfbeff772839f0e","aff7ee3744184bcbb1630eafe8f84629","2479e8dbee1a45d7ae8b3e51e8275b8b","241d7f6787774770b42389c2d5498bc7","8043e06e845d4b24a3d284d38f64957d","ba5d70e424074f7ca727b611976acffc","20e768d2df7e4acdbf2c83c379bc3198","6200d645526c47189685234af3799be0","8d26451d253e4dbfbc3c501e4189fe2f","d960404f08894c0f9ca5797b40f1e5f8","bcd1f7c10a6e4b3ba6e1312b877cc11d","8302abbb8d39408ab51f32e9edcfe383","015366f7acb2455785e46baf679c84d3","bee6302dac1042798a617edf1085315a","c8ed469098584a6b9ae9c55f95530ced","3a923c247bf94b31af70fbc98c69ec12","7eb7432e835b44c5a9794ac05772735f","5709fe89431b43849c4aebf282004392","a4a969006c0743f99981ee0fb337d047","83c9865b0df34b38ab84e31e13286d28","59cdb19064a14b51ba9a20585f2e413a","78b8f347f1d54b4684da36e39b0e493e","c698c7dfa90b41749ab1792a8f6bef99","a21945343a3e4514a6eec0bc9bdbf15b","66578b65e9ba433780f99b751900b277","b7f2e6e2621342179201daa76f325284","3d1a9b1ded8141d69c16d89fb235ab4d","30aa7e0a869643558076cc652bf8199c","778daaf6a779490e905afb3e82bf704e","05eedfd392134bcb87cdcbac508afbb3","190f703c3f554400a9476cb9a0c3fd78","66caf180d13b40719d2fe8d6908188cc","6bf5a74c4400427a8b40c06a384809fd","bc8f0bfa29b34eca803687bf97289866","aa8fe1bfedca456d8a3212ab3112fc64","396cdbb3154e4c15b66955cd7b57440f","3dea5104c53d408085b97c28132cc44e","99228dda701248d38f4fdf9a2889816c","e23fac868f624ecbb27babb7a34cef05","72dca2572ad042ff825d106e6ce5579b","db694c49c3a0475dadca0b98f016df20","21b4630774a34f7ebd3ac889d00f4a43","9a9a94a4173a4b6e84f06d013351fa90","8afda735d3a84c4f8bccc20590687bc8","f2fdc03feb8e4d718e85e8a25d061547","e35e5d2dd5b741068bdbf4285136dd95","162b41c35aff490c9e5e7e687ed39ca9","7fdc061d374b44e9b9efabfb5a847935","65dbec7671e9486d99b7f532f5a54283","6d5754c825bb417083ec9b3d1a64caea","e20bea55ceef4b0ebc8da7e42acf86e7","64fd6aade5844c89858aafb7f412b1b0","47816394ccc04a25974509d2ccac120d","8652f4a134764cfab963912450922b41","dbe07b49cf1a4f7ba83380d229230200","249239bea49d4cbfa223dc978b518b72","5067290722cb4a488b7c05f6194f2b3a","fa877e44456e4bc1bd4727aecd01686b","722fe5d558ab42b391b0733626d4cca3"]},"executionInfo":{"elapsed":45242,"status":"ok","timestamp":1730294496896,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"JfUclHr7Axcm","outputId":"b2e5915b-9b3b-4284-f705-bf6f6e458a7d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm, trange\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"127984ef3f3e4827ab3b754273f028a6","version_major":2,"version_minor":0},"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17eb7ba55f844e9ea34c3ed5af7fc4e2","version_major":2,"version_minor":0},"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6fb60459d45e4a28aa9347fd565aa58f","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04f6c584044a4ccbb52714a22db2f925","version_major":2,"version_minor":0},"text/plain":["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"79cbc512027e4a36afa452baf7535c87","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"43cf306d7ba3439f8179c25a6538ca2f","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8043e06e845d4b24a3d284d38f64957d","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a923c247bf94b31af70fbc98c69ec12","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d1a9b1ded8141d69c16d89fb235ab4d","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"99228dda701248d38f4fdf9a2889816c","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"65dbec7671e9486d99b7f532f5a54283","version_major":2,"version_minor":0},"text/plain":["1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Key Points: [('pandas library', 0.5941), ('pandas using', 0.5801), ('crowd pandas', 0.5733), ('pandas used', 0.5709), ('use pandas', 0.5566), ('pandas', 0.5546), ('pandas specifically', 0.5494), ('working pandas', 0.5373), ('science pandas', 0.534), ('using pandas', 0.5317)]\n"]}],"source":["from keybert import KeyBERT\n","\n","# Initialize KeyBERT model\n","model = KeyBERT()\n","\n","# Define transcript text\n","transcript_text = result['text']\n","\n","# Extract keywords/key points\n","keywords = model.extract_keywords(transcript_text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=10)\n","print(\"Key Points:\", keywords)\n"]},{"cell_type":"markdown","metadata":{"id":"LookQunhAHmW"},"source":["## Text to summery"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":766411,"status":"ok","timestamp":1730437055646,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"f5VwQBJU_L57","outputId":"67e53974-9e75-46c8-f195-5b7b7d7b66d2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["All of these companies have done a great job [12.520s --> 18.120s] at building foundational AI models. Today I want to show you how [23.100s --> 27.580s] you can learn AI from Nvidia and get certified by them. [28.720s --> 34.120s] Nvidia offers many courses to learn the essentials of AI. [43.680s --> 48.780s] The course is called Generative AI Explained. [43.680s --> 54.640s] that can create new content like images, text or music. [73.760s --> 77.740s] By the end of this course, you will have a good understanding of what Generative AI is capable [77.740s --> 84.300s] of and how it is shaping different industries. If you have ever worked with GPT or [84.300s --> 89.320s] any other similar large language model, you would know that sometimes it can just make stuff up. [89.640s --> 94.320s] This is also called hallucinations. [89.640s --> 94.320s] This is also called hallucinations. [94.320s --> 94.320s] This is also called hallucinations. If you have ever worked with GPT or [84.300s --> 89.320s] any other similar large language model, you would know that [94.320s] and [94.320s] [94.320s] [94.320s] [94.320s] [94.320s] [94.320s] [94.320s] [94.320s] [94.320s] [94.320s] [94.320s] [94.320s] [94.320s] [94.320s] [94.320s] [94.320s]] [94.320s] [94.320s] [94.320s] [94.320s]] [94.320s] [94.320s] [94.320s] [94.320s]] [94.320s] [94.320s]] [94.320s]]] [94.320s]] [94.320s] [94.320s] [94.320s]] [94.320s] [94.320s] [94.320s]]]]]] [94.]]] [94.]]] [94.320s] 94.] [94.320s]\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","# Load tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-large\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-large\")\n","\n","# Example text\n","text =\"\"\"Which of these is the most powerful AI company? Is it A Open AI, D Google, C Meta or D None of\n","[7.620s --> 12.520s] these? Many people argue that it's none of these. All of these companies have done a great job\n","[12.520s --> 18.120s] at building foundational AI models. But they rely on Nvidia's GPUs to train them. None of this\n","[18.120s --> 23.100s] representative AI magic would be possible if Nvidia did not exist. Today I want to show you how\n","[23.100s --> 27.580s] you can learn AI from Nvidia and get certified by them. Let's do this.\n","[28.720s --> 34.120s] Nvidia offers many courses to learn the essentials of AI. Most of these courses are free,\n","[34.420s --> 38.780s] but there are some that are paid as well. Let's start with some free courses. Let's cover\n","[38.780s --> 43.300s] the basics first. The first course I want to talk about requires no programming knowledge.\n","[43.680s --> 48.780s] The course is called Generative AI Explained. Generative AI is a type of artificial intelligence\n","[48.780s --> 54.640s] that can create new content like images, text or music. Think of tools like Dali creating images\n","[54.640s --> 60.080s] from text descriptions or chat GPT generating human like responses. The course will define\n","[60.080s --> 64.520s] Generative AI and explain how it works. It will show you all the different applications of\n","[64.520s --> 68.940s] Generative AI and tell you about the potential opportunities in the field. You will learn about\n","[68.940s --> 73.460s] the underlying technologies behind Generative AI such as neural networks and deep learning.\n","[73.760s --> 77.740s] By the end of this course, you will have a good understanding of what Generative AI is capable\n","[77.740s --> 84.300s] of and how it is shaping different industries. Moving on. If you have ever worked with GPT or\n","[84.300s --> 89.320s] any other similar large language model, you would know that sometimes it can just make stuff up.\n","[89.640s --> 94.320s] This is also called hallucinations. For this reason, most people still use Google to find\n","[94.320s --> 98.460s] information that is more factually correct. But if you are following this space closely,\n","[98.720s --> 103.120s] you would know that there are startups like perplexity that provide chat GPT like text generation,\n","[103.560s --> 108.620s] but are factually accurate. So how do they do it? They use something called Retrieval Augmented\n","[108.620s --> 114.160s] Generation or Rag. Let's break down what Retrieval Augmented Generation means. Retrieval simply\n","[114.160s --> 119.080s] means finding relevant information from a knowledge base. This could be a database of facts,\n","[119.400s --> 125.680s] articles or search results. Augmented means improving something. In this context, it means improving\n","[125.680s --> 130.640s] AI's knowledge with the help of retrieved information. Generation refers to creating new content\n","[130.640s --> 136.040s] which is what the large language model does. In simple words, with Rag, we retrieve the relevant\n","[136.040s --> 141.460s] information and use it to augment AI's knowledge before generating a response. It's an end to\n","[141.460s --> 145.780s] a architecture that combines the information retrieval component with a response generator.\n","[146.240s --> 151.620s] This approach helps to ground the AI's responses in factual information, reducing the likelihood of\n","[151.620s --> 156.700s] hallucinations. If you want to learn more about how to use Rag with LLMs, you can do this course\n","[156.700s --> 162.180s] called Augmented or LLM using the Treeval Augmented Generation by Nvidia. This course will cover\n","[162.180s --> 167.760s] the Rag components that Nvidia uses internally. It's a great way to start your journey with LLMs and\n","[167.760s --> 174.920s] Rag. In the beginning of this video, we talked about how most companies are using Nvidia CPUs to\n","[174.920s --> 181.380s] train models. But what makes Nvidia CPU so special? It's their parallel processing power. Nvidia\n","[181.380s --> 187.120s] CPUs contain thousands of smaller cores that can perform tasks simultaneously. This makes them a\n","[187.120s --> 192.220s] go-to choice for AI model training where parallel processing is critical. Let's understand why\n","[192.220s --> 197.180s] that is. Consider the task of training a large language model. These models usually have\n","[197.180s --> 202.120s] billions of parameters that need to be adjusted during the training. If you were to do it sequentially,\n","[202.580s --> 207.540s] one calculation at a time, it would take an impractically long amount of time to train. But with\n","[207.540s --> 213.360s] parallel processing, you can perform these calculations simultaneously. It significantly reduces the time\n","[213.360s --> 219.180s] to train. Compute unified device architecture or CUDA allows developers to tap into GPUs parallel\n","[219.180s --> 224.680s] processing power. In simple words, CUDA makes it easy to write code that can be run in parallel\n","[224.680s --> 229.900s] on a GPU. If you want to learn how to write parallel CUDA kernels, you can do this course called\n","[229.900s --> 235.100s] an even easier introduction to CUDA. This course will teach you how to organize parallel thread\n","[235.100s --> 240.140s] execution and efficiently manage memory. If you have played around with chat,\n","[240.140s --> 245.000s] activity or Claude, you would know that the response you get from them is as good as the questions\n","[245.000s --> 250.120s] you ask them. The art of crafting effective prompts is becoming increasingly important as AI\n","[250.120s --> 255.160s] models become more sophisticated and widely used. Asking the right questions or prompt engineering\n","[255.160s --> 259.380s] is going to become a highly valuable skill in the future. To learn prompt engineering,\n","[259.780s --> 265.200s] you can do this course called prompt engineering with Lama 2. For those who don't know, Lama is the\n","[265.200s --> 270.260s] open source slash language model from Meta. This course will teach you how to write precise prompts\n","[270.260s --> 275.020s] to achieve the desired LLM behavior. You will use techniques like few short learning where you\n","[275.020s --> 281.300s] provide examples in your prompt to guide models output. We have talked a lot about generative AI,\n","[281.720s --> 287.440s] LLMs and chat GPD so far. All these are applications of something called deep learning. Deep learning\n","[287.440s --> 292.080s] is a subset of machine learning techniques where we use artificial neural networks to learn from\n","[292.080s --> 297.380s] large amounts of data. You can get started with deep learning by using this course. All the courses\n","[297.380s --> 301.780s] that we have talked about before this one are less than 3 hours and are more high level. This\n","[301.780s --> 306.720s] course is much more hands on. This course goes a little deeper. You would need to know Python if\n","[306.720s --> 311.140s] you want to do this course. This course will teach you PyTorch which is a popular deep learning\n","[311.140s --> 317.220s] library. You will also learn convolutional neural networks or CNNs. CNNs are particularly effective for\n","[317.220s --> 322.360s] processing grid-like data like images. This course also covers transfer learning and natural\n","[322.360s --> 329.080s] language processing. All the courses we discussed can be done separately but if you like slightly\n","[329.080s --> 334.400s] more structured approach Nvidia also offers some learning paths. They have a foundational learning\n","[334.400s --> 339.660s] path for basics. This path is for those who are new to AI and want to build a solid foundation.\n","[340.000s --> 345.280s] They also have a path for generative AI and LLMs. This path goes deeper into the technology's\n","[345.280s --> 350.360s] powering latest AI breakthroughs. It covers topics like transformer architecture and diffusion\n","[350.360s --> 357.520s] models. So far we have only talked about how to learn new AI skills but what if you want to get\n","[357.520s --> 363.340s] certified for your skills. Nvidia also offers certification for different skills. These certifications\n","[363.340s --> 367.920s] can be a great way to validate your skills and stand out in the job market. For example,\n","[368.200s --> 373.340s] you can become Nvidia certified for generative AI LLMs after doing some of the courses we discussed\n","[373.340s --> 378.500s] today. They also offer some other certifications like AI Infra and Operations Certificate and\n","[378.500s --> 383.580s] Multi-Model Genie Certificate. The AI Infra and Operations Certificate can be valuable if you're\n","[383.580s --> 389.480s] interested in DevOps side of AI. The Multi-Model Genie Certification covers systems that can work with\n","[389.480s --> 394.860s] multiple types of data such as text, images and audio. Whether you are just starting out your AI\n","[394.860s --> 399.840s] journey or trying to specialize in a particular area, these courses and certifications will work as\n","[399.840s --> 404.520s] a great resource for you. I will leave a link to all of them down below. If you want to know the\n","[404.520s --> 410.320s] fastest way to learn programming using AI, watch this video. My name is Sahil and I'll see you in the\n","[410.320s --> 410.800s] next one.\"\"\"\n","\n","# Tokenize input\n","inputs = tokenizer(text, max_length=1024, return_tensors=\"pt\", truncation=True)\n","\n","# Generate summary\n","summary_ids = model.generate(inputs[\"input_ids\"], max_length=1000, min_length=500, length_penalty=1.0, num_beams=7, early_stopping=False)\n","\n","# Decode summary\n","# Decode summary with explicit setting\n","summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","\n","print(summary)\n"]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9XMGKTQUzqbq","executionInfo":{"status":"ok","timestamp":1730433464395,"user_tz":-330,"elapsed":2235,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"}},"outputId":"d2ebf95b-17a9-4434-c171-7baf7a6e1017"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"]}]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n","\n","ARTICLE = \"\"\"Which of these is the most powerful AI company? Is it A Open AI, D Google, C Meta or D None of\n","[7.620s --> 12.520s] these? Many people argue that it's none of these. All of these companies have done a great job\n","[12.520s --> 18.120s] at building foundational AI models. But they rely on Nvidia's GPUs to train them. None of this\n","[18.120s --> 23.100s] representative AI magic would be possible if Nvidia did not exist. Today I want to show you how\n","[23.100s --> 27.580s] you can learn AI from Nvidia and get certified by them. Let's do this.\n","[28.720s --> 34.120s] Nvidia offers many courses to learn the essentials of AI. Most of these courses are free,\n","[34.420s --> 38.780s] but there are some that are paid as well. Let's start with some free courses. Let's cover\n","[38.780s --> 43.300s] the basics first. The first course I want to talk about requires no programming knowledge.\n","[43.680s --> 48.780s] The course is called Generative AI Explained. Generative AI is a type of artificial intelligence\n","[48.780s --> 54.640s] that can create new content like images, text or music. Think of tools like Dali creating images\n","[54.640s --> 60.080s] from text descriptions or chat GPT generating human like responses. The course will define\n","[60.080s --> 64.520s] Generative AI and explain how it works. It will show you all the different applications of\n","[64.520s --> 68.940s] Generative AI and tell you about the potential opportunities in the field. You will learn about\n","[68.940s --> 73.460s] the underlying technologies behind Generative AI such as neural networks and deep learning.\n","[73.760s --> 77.740s] By the end of this course, you will have a good understanding of what Generative AI is capable\n","[77.740s --> 84.300s] of and how it is shaping different industries. Moving on. If you have ever worked with GPT or\n","[84.300s --> 89.320s] any other similar large language model, you would know that sometimes it can just make stuff up.\n","[89.640s --> 94.320s] This is also called hallucinations. For this reason, most people still use Google to find\n","[94.320s --> 98.460s] information that is more factually correct. But if you are following this space closely,\n","[98.720s --> 103.120s] you would know that there are startups like perplexity that provide chat GPT like text generation,\n","[103.560s --> 108.620s] but are factually accurate. So how do they do it? They use something called Retrieval Augmented\n","[108.620s --> 114.160s] Generation or Rag. Let's break down what Retrieval Augmented Generation means. Retrieval simply\n","[114.160s --> 119.080s] means finding relevant information from a knowledge base. This could be a database of facts,\n","[119.400s --> 125.680s] articles or search results. Augmented means improving something. In this context, it means improving\n","[125.680s --> 130.640s] AI's knowledge with the help of retrieved information. Generation refers to creating new content\n","[130.640s --> 136.040s] which is what the large language model does. In simple words, with Rag, we retrieve the relevant\n","[136.040s --> 141.460s] information and use it to augment AI's knowledge before generating a response. It's an end to\n","[141.460s --> 145.780s] a architecture that combines the information retrieval component with a response generator.\n","[146.240s --> 151.620s] This approach helps to ground the AI's responses in factual information, reducing the likelihood of\n","[151.620s --> 156.700s] hallucinations. If you want to learn more about how to use Rag with LLMs, you can do this course\n","[156.700s --> 162.180s] called Augmented or LLM using the Treeval Augmented Generation by Nvidia. This course will cover\n","[162.180s --> 167.760s] the Rag components that Nvidia uses internally. It's a great way to start your journey with LLMs and\n","[167.760s --> 174.920s] Rag. In the beginning of this video, we talked about how most companies are using Nvidia CPUs to\n","[174.920s --> 181.380s] train models. But what makes Nvidia CPU so special? It's their parallel processing power. Nvidia\n","[181.380s --> 187.120s] CPUs contain thousands of smaller cores that can perform tasks simultaneously. This makes them a\n","[187.120s --> 192.220s] go-to choice for AI model training where parallel processing is critical. Let's understand why\n","[192.220s --> 197.180s] that is. Consider the task of training a large language model. These models usually have\n","[197.180s --> 202.120s] billions of parameters that need to be adjusted during the training. If you were to do it sequentially,\n","[202.580s --> 207.540s] one calculation at a time, it would take an impractically long amount of time to train. But with\n","[207.540s --> 213.360s] parallel processing, you can perform these calculations simultaneously. It significantly reduces the time\n","[213.360s --> 219.180s] to train. Compute unified device architecture or CUDA allows developers to tap into GPUs parallel\n","[219.180s --> 224.680s] processing power. In simple words, CUDA makes it easy to write code that can be run in parallel\n","[224.680s --> 229.900s] on a GPU. If you want to learn how to write parallel CUDA kernels, you can do this course called\n","[229.900s --> 235.100s] an even easier introduction to CUDA. This course will teach you how to organize parallel thread\n","[235.100s --> 240.140s] execution and efficiently manage memory. If you have played around with chat,\n","[240.140s --> 245.000s] activity or Claude, you would know that the response you get from them is as good as the questions\n","[245.000s --> 250.120s] you ask them. The art of crafting effective prompts is becoming increasingly important as AI\n","[250.120s --> 255.160s] models become more sophisticated and widely used. Asking the right questions or prompt engineering\n","[255.160s --> 259.380s] is going to become a highly valuable skill in the future. To learn prompt engineering,\n","[259.780s --> 265.200s] you can do this course called prompt engineering with Lama 2. For those who don't know, Lama is the\n","[265.200s --> 270.260s] open source slash language model from Meta. This course will teach you how to write precise prompts\n","[270.260s --> 275.020s] to achieve the desired LLM behavior. You will use techniques like few short learning where you\n","[275.020s --> 281.300s] provide examples in your prompt to guide models output. We have talked a lot about generative AI,\n","[281.720s --> 287.440s] LLMs and chat GPD so far. All these are applications of something called deep learning. Deep learning\n","[287.440s --> 292.080s] is a subset of machine learning techniques where we use artificial neural networks to learn from\n","[292.080s --> 297.380s] large amounts of data. You can get started with deep learning by using this course. All the courses\n","[297.380s --> 301.780s] that we have talked about before this one are less than 3 hours and are more high level. This\n","[301.780s --> 306.720s] course is much more hands on. This course goes a little deeper. You would need to know Python if\n","[306.720s --> 311.140s] you want to do this course. This course will teach you PyTorch which is a popular deep learning\n","[311.140s --> 317.220s] library. You will also learn convolutional neural networks or CNNs. CNNs are particularly effective for\n","[317.220s --> 322.360s] processing grid-like data like images. This course also covers transfer learning and natural\n","[322.360s --> 329.080s] language processing. All the courses we discussed can be done separately but if you like slightly\n","[329.080s --> 334.400s] more structured approach Nvidia also offers some learning paths. They have a foundational learning\n","[334.400s --> 339.660s] path for basics. This path is for those who are new to AI and want to build a solid foundation.\n","[340.000s --> 345.280s] They also have a path for generative AI and LLMs. This path goes deeper into the technology's\n","[345.280s --> 350.360s] powering latest AI breakthroughs. It covers topics like transformer architecture and diffusion\n","[350.360s --> 357.520s] models. So far we have only talked about how to learn new AI skills but what if you want to get\n","[357.520s --> 363.340s] certified for your skills. Nvidia also offers certification for different skills. These certifications\n","[363.340s --> 367.920s] can be a great way to validate your skills and stand out in the job market. For example,\n","[368.200s --> 373.340s] you can become Nvidia certified for generative AI LLMs after doing some of the courses we discussed\n","[373.340s --> 378.500s] today. They also offer some other certifications like AI Infra and Operations Certificate and\n","[378.500s --> 383.580s] Multi-Model Genie Certificate. The AI Infra and Operations Certificate can be valuable if you're\n","[383.580s --> 389.480s] interested in DevOps side of AI. The Multi-Model Genie Certification covers systems that can work with\n","[389.480s --> 394.860s] multiple types of data such as text, images and audio. Whether you are just starting out your AI\n","[394.860s --> 399.840s] journey or trying to specialize in a particular area, these courses and certifications will work as\n","[399.840s --> 404.520s] a great resource for you. I will leave a link to all of them down below. If you want to know the\n","[404.520s --> 410.320s] fastest way to learn programming using AI, watch this video. My name is Sahil and I'll see you in the\n","[410.320s --> 410.800s] next one.\"\"\"\n","print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"id":"v_fOkYPw0i-a","executionInfo":{"status":"error","timestamp":1730433478352,"user_tz":-330,"elapsed":5172,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"}},"outputId":"0696b7a6-1c0d-4548-f165-478b2fd3dd4d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"]},{"output_type":"error","ename":"IndexError","evalue":"index out of range in self","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-d28414435c7c>\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m404.520\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m->\u001b[0m \u001b[0;36m410.320\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mfastest\u001b[0m \u001b[0mway\u001b[0m \u001b[0mto\u001b[0m \u001b[0mlearn\u001b[0m \u001b[0mprogramming\u001b[0m \u001b[0musing\u001b[0m \u001b[0mAI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwatch\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mMy\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mSahil\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mI\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mll\u001b[0m \u001b[0msee\u001b[0m \u001b[0myou\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m [410.320s --> 410.800s] next one.\"\"\"\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARTICLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m130\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \"\"\"\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         if (\n\u001b[1;32m    169\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1255\u001b[0m             )\n\u001b[1;32m   1256\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1162\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         )\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"encoder_outputs\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m             \u001b[0;31m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[1;32m   1746\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values_length)\u001b[0m\n\u001b[1;32m    113\u001b[0m         ).expand(bsz, -1)\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index out of range in self"]}]},{"cell_type":"code","source":["summary=summarizer(result['text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304},"id":"gIrQcKGaz9BX","executionInfo":{"status":"error","timestamp":1730433147991,"user_tz":-330,"elapsed":756,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"}},"outputId":"2408a34b-6746-48ea-9bea-22a133fae5a2"},"execution_count":27,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"index out of range in self","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-b05ba31e44c2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \"\"\"\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         if (\n\u001b[1;32m    169\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1255\u001b[0m             )\n\u001b[1;32m   1256\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1162\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         )\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"encoder_outputs\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m             \u001b[0;31m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[1;32m   1746\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values_length)\u001b[0m\n\u001b[1;32m    113\u001b[0m         ).expand(bsz, -1)\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index out of range in self"]}]},{"cell_type":"code","source":["# Your transcript text\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n"],"metadata":{"id":"kcFT4lTmARnG","executionInfo":{"status":"ok","timestamp":1730435689296,"user_tz":-330,"elapsed":819,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"executionInfo":{"elapsed":481,"status":"error","timestamp":1730435697961,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"N3fcMnJG_L8J","outputId":"2e0ed0f1-6bb2-47e9-cdfe-486a34f6642d"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-fe365bea2c2e>\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"summarize\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtranscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming you want to use CUDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# Generate the summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2903\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 )\n\u001b[0;32m-> 2905\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2907\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}],"source":["\n","import torch\n","transcript = \"\"\"Which of these is the most powerful AI company? Is it A Open AI, D Google, C Meta or D None of\n","[7.620s --> 12.520s] these? Many people argue that it's none of these. All of these companies have done a great job\n","[12.520s --> 18.120s] at building foundational AI models. But they rely on Nvidia's GPUs to train them. None of this\n","[18.120s --> 23.100s] representative AI magic would be possible if Nvidia did not exist. Today I want to show you how\n","[23.100s --> 27.580s] you can learn AI from Nvidia and get certified by them. Let's do this.\n","[28.720s --> 34.120s] Nvidia offers many courses to learn the essentials of AI. Most of these courses are free,\n","[34.420s --> 38.780s] but there are some that are paid as well. Let's start with some free courses. Let's cover\n","[38.780s --> 43.300s] the basics first. The first course I want to talk about requires no programming knowledge.\n","[43.680s --> 48.780s] The course is called Generative AI Explained. Generative AI is a type of artificial intelligence\n","[48.780s --> 54.640s] that can create new content like images, text or music. Think of tools like Dali creating images\n","[54.640s --> 60.080s] from text descriptions or chat GPT generating human like responses. The course will define\n","[60.080s --> 64.520s] Generative AI and explain how it works. It will show you all the different applications of\n","[64.520s --> 68.940s] Generative AI and tell you about the potential opportunities in the field. You will learn about\n","[68.940s --> 73.460s] the underlying technologies behind Generative AI such as neural networks and deep learning.\n","[73.760s --> 77.740s] By the end of this course, you will have a good understanding of what Generative AI is capable\n","[77.740s --> 84.300s] of and how it is shaping different industries. Moving on. If you have ever worked with GPT or\n","[84.300s --> 89.320s] any other similar large language model, you would know that sometimes it can just make stuff up.\n","[89.640s --> 94.320s] This is also called hallucinations. For this reason, most people still use Google to find\n","[94.320s --> 98.460s] information that is more factually correct. But if you are following this space closely,\n","[98.720s --> 103.120s] you would know that there are startups like perplexity that provide chat GPT like text generation,\n","[103.560s --> 108.620s] but are factually accurate. So how do they do it? They use something called Retrieval Augmented\n","[108.620s --> 114.160s] Generation or Rag. Let's break down what Retrieval Augmented Generation means. Retrieval simply\n","[114.160s --> 119.080s] means finding relevant information from a knowledge base. This could be a database of facts,\n","[119.400s --> 125.680s] articles or search results. Augmented means improving something. In this context, it means improving\n","[125.680s --> 130.640s] AI's knowledge with the help of retrieved information. Generation refers to creating new content\n","[130.640s --> 136.040s] which is what the large language model does. In simple words, with Rag, we retrieve the relevant\n","[136.040s --> 141.460s] information and use it to augment AI's knowledge before generating a response. It's an end to\n","[141.460s --> 145.780s] a architecture that combines the information retrieval component with a response generator.\n","[146.240s --> 151.620s] This approach helps to ground the AI's responses in factual information, reducing the likelihood of\n","[151.620s --> 156.700s] hallucinations. If you want to learn more about how to use Rag with LLMs, you can do this course\n","[156.700s --> 162.180s] called Augmented or LLM using the Treeval Augmented Generation by Nvidia. This course will cover\n","[162.180s --> 167.760s] the Rag components that Nvidia uses internally. It's a great way to start your journey with LLMs and\n","[167.760s --> 174.920s] Rag. In the beginning of this video, we talked about how most companies are using Nvidia CPUs to\n","[174.920s --> 181.380s] train models. But what makes Nvidia CPU so special? It's their parallel processing power. Nvidia\n","[181.380s --> 187.120s] CPUs contain thousands of smaller cores that can perform tasks simultaneously. This makes them a\n","[187.120s --> 192.220s] go-to choice for AI model training where parallel processing is critical. Let's understand why\n","[192.220s --> 197.180s] that is. Consider the task of training a large language model. These models usually have\n","[197.180s --> 202.120s] billions of parameters that need to be adjusted during the training. If you were to do it sequentially,\n","[202.580s --> 207.540s] one calculation at a time, it would take an impractically long amount of time to train. But with\n","[207.540s --> 213.360s] parallel processing, you can perform these calculations simultaneously. It significantly reduces the time\n","[213.360s --> 219.180s] to train. Compute unified device architecture or CUDA allows developers to tap into GPUs parallel\n","[219.180s --> 224.680s] processing power. In simple words, CUDA makes it easy to write code that can be run in parallel\n","[224.680s --> 229.900s] on a GPU. If you want to learn how to write parallel CUDA kernels, you can do this course called\n","[229.900s --> 235.100s] an even easier introduction to CUDA. This course will teach you how to organize parallel thread\n","[235.100s --> 240.140s] execution and efficiently manage memory. If you have played around with chat,\n","[240.140s --> 245.000s] activity or Claude, you would know that the response you get from them is as good as the questions\n","[245.000s --> 250.120s] you ask them. The art of crafting effective prompts is becoming increasingly important as AI\n","[250.120s --> 255.160s] models become more sophisticated and widely used. Asking the right questions or prompt engineering\n","[255.160s --> 259.380s] is going to become a highly valuable skill in the future. To learn prompt engineering,\n","[259.780s --> 265.200s] you can do this course called prompt engineering with Lama 2. For those who don't know, Lama is the\n","[265.200s --> 270.260s] open source slash language model from Meta. This course will teach you how to write precise prompts\n","[270.260s --> 275.020s] to achieve the desired LLM behavior. You will use techniques like few short learning where you\n","[275.020s --> 281.300s] provide examples in your prompt to guide models output. We have talked a lot about generative AI,\n","[281.720s --> 287.440s] LLMs and chat GPD so far. All these are applications of something called deep learning. Deep learning\n","[287.440s --> 292.080s] is a subset of machine learning techniques where we use artificial neural networks to learn from\n","[292.080s --> 297.380s] large amounts of data. You can get started with deep learning by using this course. All the courses\n","[297.380s --> 301.780s] that we have talked about before this one are less than 3 hours and are more high level. This\n","[301.780s --> 306.720s] course is much more hands on. This course goes a little deeper. You would need to know Python if\n","[306.720s --> 311.140s] you want to do this course. This course will teach you PyTorch which is a popular deep learning\n","[311.140s --> 317.220s] library. You will also learn convolutional neural networks or CNNs. CNNs are particularly effective for\n","[317.220s --> 322.360s] processing grid-like data like images. This course also covers transfer learning and natural\n","[322.360s --> 329.080s] language processing. All the courses we discussed can be done separately but if you like slightly\n","[329.080s --> 334.400s] more structured approach Nvidia also offers some learning paths. They have a foundational learning\n","[334.400s --> 339.660s] path for basics. This path is for those who are new to AI and want to build a solid foundation.\n","[340.000s --> 345.280s] They also have a path for generative AI and LLMs. This path goes deeper into the technology's\n","[345.280s --> 350.360s] powering latest AI breakthroughs. It covers topics like transformer architecture and diffusion\n","[350.360s --> 357.520s] models. So far we have only talked about how to learn new AI skills but what if you want to get\n","[357.520s --> 363.340s] certified for your skills. Nvidia also offers certification for different skills. These certifications\n","[363.340s --> 367.920s] can be a great way to validate your skills and stand out in the job market. For example,\n","[368.200s --> 373.340s] you can become Nvidia certified for generative AI LLMs after doing some of the courses we discussed\n","[373.340s --> 378.500s] today. They also offer some other certifications like AI Infra and Operations Certificate and\n","[378.500s --> 383.580s] Multi-Model Genie Certificate. The AI Infra and Operations Certificate can be valuable if you're\n","[383.580s --> 389.480s] interested in DevOps side of AI. The Multi-Model Genie Certification covers systems that can work with\n","[389.480s --> 394.860s] multiple types of data such as text, images and audio. Whether you are just starting out your AI\n","[394.860s --> 399.840s] journey or trying to specialize in a particular area, these courses and certifications will work as\n","[399.840s --> 404.520s] a great resource for you. I will leave a link to all of them down below. If you want to know the\n","[404.520s --> 410.320s] fastest way to learn programming using AI, watch this video. My name is Sahil and I'll see you in the\n","[410.320s --> 410.800s] next one.\"\"\"\n","# Tokenize the input text and set max length (you can split long text if needed)\n","inputs = tokenizer.encode(\"summarize\" + transcript, return_tensors=\"pt\", max_length=1024, truncation=True)\n","\n","# Generate the summary\n","summary_ids = model.generate(\n","    inputs,\n","    max_length=1000,         # Allow a longer summary\n","    min_length=500,         # Ensure a substantial summary length\n","    length_penalty=1,     # Lower penalty to encourage longer output\n","    num_beams=6,            # Increase beams to explore more summary options\n","    early_stopping=True,\n",")\n","# Decode the summary\n","summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","\n","# Display the summary\n","print(\"Summary:\", summary)\n","summery=[]\n","for sent in summary.split('.'):\n","  summery.append(sent)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36660,"status":"ok","timestamp":1730435057188,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"whBtHXv6FPo5","outputId":"23a9fde5-5ff6-4a98-c1c9-ddd442152d4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["['一个名为Nvidia的AI公司,它是一家Open AI,D谷歌,C Meta或D None of 7.620s。该公司在建造基础al AI models时,依赖Nvigia的GPUs来训练它们。此外,该公司还提供了一些免费的AI课程,包括免费的课程、免费课程和通用erative AI Explained。']\n"]}],"source":["from transformers import T5ForConditionalGeneration, T5Tokenizer\n","\n","device = 'cpu' #or 'cpu' for translate on cpu\n","\n","model_name = 'utrobinmv/t5_summary_en_ru_zh_base_2048'\n","model = T5ForConditionalGeneration.from_pretrained(model_name)\n","model.eval()\n","model.to(device)\n","tokenizer = T5Tokenizer.from_pretrained(model_name)\n","\n","text = \"\"\"Which of these is the most powerful AI company? Is it A Open AI, D Google, C Meta or D None of\n","[7.620s --> 12.520s] these? Many people argue that it's none of these. All of these companies have done a great job\n","[12.520s --> 18.120s] at building foundational AI models. But they rely on Nvidia's GPUs to train them. None of this\n","[18.120s --> 23.100s] representative AI magic would be possible if Nvidia did not exist. Today I want to show you how\n","[23.100s --> 27.580s] you can learn AI from Nvidia and get certified by them. Let's do this.\n","[28.720s --> 34.120s] Nvidia offers many courses to learn the essentials of AI. Most of these courses are free,\n","[34.420s --> 38.780s] but there are some that are paid as well. Let's start with some free courses. Let's cover\n","[38.780s --> 43.300s] the basics first. The first course I want to talk about requires no programming knowledge.\n","[43.680s --> 48.780s] The course is called Generative AI Explained. Generative AI is a type of artificial intelligence\n","\"\"\"\n","\n","\n","\n","# text big summary generate\n","prefix = 'summary big: '\n","src_text = prefix + text\n","input_ids = tokenizer(src_text, return_tensors=\"pt\")\n","\n","generated_tokens = model.generate(**input_ids.to(device))\n","\n","result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","print(result)\n","#YouTube has said it will remove more than 1,500 videos of Covid vaccines from its platform in a bid to tackle the spread of misinformation about the jabs.\n"]},{"cell_type":"markdown","metadata":{"id":"qUCgJV8wAZF_"},"source":["## summery and extract time stamps"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7458,"status":"ok","timestamp":1730295201114,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"WW2o_7Rr_MAt","outputId":"e1e89a8a-4d73-41c5-a4f2-2d2aef7e1965"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.0+cu121)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"]}],"source":["!pip install sentence-transformers\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5397,"status":"ok","timestamp":1730295208494,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"},"user_tz":-330},"id":"F-SzjPDFAeiF","outputId":"d0de19e4-c563-4e0d-899f-5f612dcc9de5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Summary Point 1: 'Pandas is a powerful library that's used all around the world for data science'\n","   Closest match in transcript (Timestamp: 14.56s - 16.0s): 'Pandas for data science.'\n","\n","Summary Point 2: ' It helps you work, process, and filter tabular data so you can get your machine learning models out there a whole heap faster'\n","   Closest match in transcript (Timestamp: 21.52s - 24.240000000000002s): 'It helps you work, process, and filter tabular data'\n","\n","Summary Point 3: ' In this video, we're going to go through everything you need to get up and started super fast to work with pandas'\n","   Closest match in transcript (Timestamp: 0.0s - 2.0s): 'So today we're going to be taking a look at pandas.'\n","\n","Summary Point 4: ' We're also going to be treating this like a bit of a crash course'\n","   Closest match in transcript (Timestamp: 49.120000000000005s - 51.68s): 'We're also going to be treating this like a bit of a crash course.'\n","\n","Summary Point 5: ' So we'll read in some tabularData and we'll go through all the steps that you need in order to use pandas for dataScience'\n","   Closest match in transcript (Timestamp: 14.56s - 16.0s): 'Pandas for data science.'\n","\n","Summary Point 6: ' And last but not least, we'll take a look at how we can output all of our resulting data to a Jupyter notebook'\n","   Closest match in transcript (Timestamp: 64.0s - 66.8s): 'and we're going to be coding inside of a Jupyter notebook'\n","\n","Summary Point 7: ' The video will be broken up into four key sections: reading data frames, working with all the data within them, dropping rows and working with columns, and visualizing the data frame'\n","   Closest match in transcript (Timestamp: 85.03999999999999s - 88.24s): 'reading data frames, and specifically, we're going to be taking a look at how to work'\n","\n","Summary Point 8: ' It will also cover how to create a data frame from a dictionary, which is just a different Python data dictionary'\n","   Closest match in transcript (Timestamp: 232.4s - 237.68s): 'So a dictionary is just a different Python data type, but it allows us to work with a bunch of'\n","\n","Summary Point 9: ' The final part of the video will show you how to use the pandas library to create data frames from a CSV and work with all of your data within it'\n","   Closest match in transcript (Timestamp: 130.0s - 135.04s): 'So this allows us to create a data frame from a CSV and work with it in pandas.'\n","\n","Summary Point 10: ' It'll also give you some tips and tricks on how to get started using pandas in your own data science lab or on your own computer as a data scientist'\n","   Closest match in transcript (Timestamp: 14.56s - 16.0s): 'Pandas for data science.'\n","\n","Summary Point 11: ' It's also a great way to learn more about pandas and how it's being used by other data scientists all over the world to train your models and train your machine Learning models'\n","   Closest match in transcript (Timestamp: 54.96s - 57.120000000000005s): 'that you need in order to work with pandas.'\n","\n","Summary Point 12: ' The full video is available online at: http://www'\n","   Closest match in transcript (Timestamp: 1363.44s - 1366.88s): 'Thanks so much for tuning in guys. Hopefully you found this video useful. If you did be'\n","\n","Summary Point 13: 'cnn'\n","   Closest match in transcript (Timestamp: 97.6s - 99.75999999999999s): 'all of our resulting data.'\n","\n","Summary Point 14: 'com/2013/01/29/pandas-crash-course-how-to-work-with- pandas-for-data-scientists-in-your-laptop-and-on-the-phone-and what-you-should-do-when-you're-training-a-machine-learning-model-that-can't-be-seen-by-any-one-at-all-where-it-is-out-there-or-what-it's-called-by your-researchers-around-the world and how-you can-use-it to-train-a machine-Learning-model in your laboratory to the-back-of-your home-office'\n","   Closest match in transcript (Timestamp: 54.96s - 57.120000000000005s): 'that you need in order to work with pandas.'\n","\n","Summary Point 15: ''\n","   Closest match in transcript (Timestamp: 162.72s - 163.28s): 'All righty.'\n","\n"]}],"source":["from sentence_transformers import SentenceTransformer, util\n","import numpy as np\n","\n","# Initialize the model for embeddings\n","model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight model, change as needed\n","\n","# Example transcript with timestamps\n","transcript = original_transcript\n","\n","# Summarized bullet points (from a summarizer like BART)\n","summary = summery\n","\n","# Step 1: Generate embeddings for transcript and summary\n","transcript_sentences = [sent[2] for sent in transcript]\n","transcript_embeddings = model.encode(transcript_sentences, convert_to_tensor=True)\n","summary_embeddings = model.encode(summary, convert_to_tensor=True)\n","\n","# Step 2: Calculate similarity and find the closest match for each summary point\n","for i, sum_emb in enumerate(summary_embeddings):\n","    # Compute cosine similarities between the summary and all transcript sentences\n","    cosine_scores = util.cos_sim(sum_emb, transcript_embeddings)[0]\n","\n","    # Get the index of the transcript sentence with the highest similarity score\n","    most_similar_idx = np.argmax(cosine_scores.cpu().numpy())\n","\n","    # Extract timestamp of the most similar sentence\n","    timestamp = transcript[most_similar_idx][:2]\n","    print(f\"Summary Point {i+1}: '{summary[i]}'\")\n","    print(f\"   Closest match in transcript (Timestamp: {timestamp[0]}s - {timestamp[1]}s): '{transcript[most_similar_idx][2]}'\")\n","    print()"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"28SUGJ9bAxfe","executionInfo":{"status":"ok","timestamp":1730437732235,"user_tz":-330,"elapsed":739,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"}}},"outputs":[],"source":["from transformers import pipeline\n","import os\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# Use the Pegasus model explicitly\n","summarizer = pipeline(\"summarization\", model=\"google/pegasus-large\",device=0)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vb3UkEwYF0bz","executionInfo":{"status":"ok","timestamp":1730438110510,"user_tz":-330,"elapsed":9043,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"}},"outputId":"ad6676ee-4632-4212-9b52-5a16717bee6d"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"]}]},{"cell_type":"code","source":["text=\"\"\"The Age of Artificial Intelligence: A Comprehensive Overview\n","Introduction\n","Artificial Intelligence (AI) has become one of the most transformative technologies of the 21st century, influencing almost every industry and aspect of our daily lives. From personal assistants like Siri and Alexa to large-scale applications in medicine, finance, and autonomous vehicles, AI is shaping the way we interact with technology and our environment. But what exactly is AI, how did it come about, and where is it headed?\n","\n","What is Artificial Intelligence?\n","AI refers to the simulation of human intelligence in machines designed to perform tasks that typically require human intelligence. These tasks include problem-solving, decision-making, understanding language, recognizing patterns, and learning from experience. Broadly, AI is categorized into three levels:\n","\n","Artificial Narrow Intelligence (ANI): This is AI specialized for a single task, like language translation or facial recognition. Most AI systems today fall into this category.\n","\n","Artificial General Intelligence (AGI): Also known as \"strong AI,\" this level of AI would be capable of performing any intellectual task that a human can. It remains a theoretical concept and has not been achieved yet.\n","\n","Artificial Superintelligence (ASI): This is a hypothetical level where AI surpasses human intelligence in all aspects. The ethical and existential implications of ASI make it a topic of intense debate among researchers and ethicists.\n","\n","A Brief History of AI\n","AI as a formal field of study emerged in the 1950s, with the Dartmouth Conference in 1956 marking the birth of AI as a research domain. The conference introduced concepts like machine learning, where computers could improve from experience without being explicitly programmed.\n","\n","The journey of AI can be divided into several phases:\n","\n","1950s-1970s: The Early Years - Initial enthusiasm led to the development of basic algorithms and the first AI programs, including early attempts at natural language processing and simple robots.\n","\n","1970s-1990s: The AI Winters - Following periods of overhyped expectations, AI research faced cuts in funding and interest due to limitations in computing power and unmet expectations.\n","\n","1990s-2010: A Period of Revival - Advancements in computing, especially in hardware, enabled AI research to progress. The development of algorithms like support vector machines and early neural networks led to breakthroughs in image recognition and data analysis.\n","\n","2010-Present: The Deep Learning Revolution - The advent of deep learning, powered by increased computational capacity and massive datasets, marked a transformative phase. AI applications like deep neural networks for image and speech recognition, machine translation, and generative models flourished.\n","\n","Key Technologies in AI\n","AI is built on several key technologies:\n","\n","Machine Learning (ML): ML enables computers to learn from data. Supervised learning, unsupervised learning, and reinforcement learning are the primary types of ML.\n","\n","Deep Learning (DL): A subset of ML, deep learning uses neural networks with many layers to analyze complex data patterns. Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) are examples of deep learning architectures.\n","\n","Natural Language Processing (NLP): NLP allows computers to understand, interpret, and generate human language. GPT, BERT, and T5 are state-of-the-art NLP models used in translation, chatbots, and sentiment analysis.\n","\n","Computer Vision: This field enables computers to understand and process visual information, powering technologies like facial recognition, autonomous vehicles, and medical image analysis.\n","\n","\"\"\""],"metadata":{"id":"_jU1bx_dGAgO","executionInfo":{"status":"ok","timestamp":1730438110511,"user_tz":-330,"elapsed":3,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["summery=summarizer(text,max_length=500, min_length=100, do_sample=False,)"],"metadata":{"id":"zg30sd_yGiTl","executionInfo":{"status":"ok","timestamp":1730438190593,"user_tz":-330,"elapsed":77941,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["summery"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1MLFqSvqHByf","executionInfo":{"status":"ok","timestamp":1730438194578,"user_tz":-330,"elapsed":794,"user":{"displayName":"MAHI REDDY","userId":"07425330453556974324"}},"outputId":"17f42996-474c-496f-d053-b369e851b3ac"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'summary_text': 'Broadly, AI is categorized into three levels: Artificial Narrow Intelligence (ANI): This is AI specialized for a single task, like language translation or facial recognition. The journey of AI can be divided into several phases: 1950s-1970s: The Early Years - Initial enthusiasm led to the development of basic algorithms and the first AI programs, including early attempts at natural language processing and simple robots. The development of algorithms like support vector machines and early neural networks led to breakthroughs in image recognition and data analysis.'}]"]},"metadata":{},"execution_count":21}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyPLkJWbcLyGhtitPxkRObtx"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"015366f7acb2455785e46baf679c84d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"01c1a10e09c242ca8dfbeff772839f0e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04f6c584044a4ccbb52714a22db2f925":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dca31c359b99468fa78672b20788fb72","IPY_MODEL_cc2243402ff0428291c9af8679a8c745","IPY_MODEL_0c4758ebafc14e97876084f64e582a1d"],"layout":"IPY_MODEL_48cfdae9c29242b1ac7b2cec01176832"}},"05eedfd392134bcb87cdcbac508afbb3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_396cdbb3154e4c15b66955cd7b57440f","placeholder":"​","style":"IPY_MODEL_3dea5104c53d408085b97c28132cc44e","value":" 466k/466k [00:00&lt;00:00, 18.1MB/s]"}},"09803a494e4642509c2a4218219c71ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c4758ebafc14e97876084f64e582a1d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a48b47e527b443882b6dd2f608a35eb","placeholder":"​","style":"IPY_MODEL_6abf15c7d58949348593846555ae1e15","value":" 53.0/53.0 [00:00&lt;00:00, 2.70kB/s]"}},"0d39af349a0a46dbb3fa7c02fb860732":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2479e8dbee1a45d7ae8b3e51e8275b8b","placeholder":"​","style":"IPY_MODEL_241d7f6787774770b42389c2d5498bc7","value":" 90.9M/90.9M [00:00&lt;00:00, 154MB/s]"}},"0d866581898445c29069342b81af9f59":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"105a30493f56442eb2075f5a88d3c236":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"127984ef3f3e4827ab3b754273f028a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30d30b86d98d443d8cdfddaa20a1edfe","IPY_MODEL_64050d74a49f49cdb5a7e89906bd93d0","IPY_MODEL_1977a2744a294877a4474b8288e56285"],"layout":"IPY_MODEL_724d95e47dcb4511b7aea7f5676c2e45"}},"13771a3cbabc4e7d9fd707ce3d43f374":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14d305f2a393475f84f8f8da471fe909":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1584976b17864a84aa1369f6e09a0ecd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15f4d1188627496a81792d03a95183bf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"162b41c35aff490c9e5e7e687ed39ca9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17eb7ba55f844e9ea34c3ed5af7fc4e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_567a69c762354c0689ec7b21df0614ad","IPY_MODEL_aa6dbe0fb9814820874e6627080b407a","IPY_MODEL_d452a3d313004c3b82bde3b8b0a3124d"],"layout":"IPY_MODEL_15f4d1188627496a81792d03a95183bf"}},"190f703c3f554400a9476cb9a0c3fd78":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1977a2744a294877a4474b8288e56285":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_837ef3cb84b3439390b214b59848b670","placeholder":"​","style":"IPY_MODEL_59ea89db489f4ad5b940fa0eedde7eb2","value":" 349/349 [00:00&lt;00:00, 12.5kB/s]"}},"1ed91e4f349d4f5f942df73fd74da144":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20e768d2df7e4acdbf2c83c379bc3198":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8302abbb8d39408ab51f32e9edcfe383","max":350,"min":0,"orientation":"horizontal","style":"IPY_MODEL_015366f7acb2455785e46baf679c84d3","value":350}},"21b4630774a34f7ebd3ac889d00f4a43":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2315022199bd45f096b706fc77d845ef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23aba67299ab4292aed339a15eeb53c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"241d7f6787774770b42389c2d5498bc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2479e8dbee1a45d7ae8b3e51e8275b8b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"249239bea49d4cbfa223dc978b518b72":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30aa7e0a869643558076cc652bf8199c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66caf180d13b40719d2fe8d6908188cc","placeholder":"​","style":"IPY_MODEL_6bf5a74c4400427a8b40c06a384809fd","value":"tokenizer.json: 100%"}},"30d30b86d98d443d8cdfddaa20a1edfe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa9499cb907348649972ec22e53186f0","placeholder":"​","style":"IPY_MODEL_14d305f2a393475f84f8f8da471fe909","value":"modules.json: 100%"}},"396cdbb3154e4c15b66955cd7b57440f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a923c247bf94b31af70fbc98c69ec12":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7eb7432e835b44c5a9794ac05772735f","IPY_MODEL_5709fe89431b43849c4aebf282004392","IPY_MODEL_a4a969006c0743f99981ee0fb337d047"],"layout":"IPY_MODEL_83c9865b0df34b38ab84e31e13286d28"}},"3d1a9b1ded8141d69c16d89fb235ab4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30aa7e0a869643558076cc652bf8199c","IPY_MODEL_778daaf6a779490e905afb3e82bf704e","IPY_MODEL_05eedfd392134bcb87cdcbac508afbb3"],"layout":"IPY_MODEL_190f703c3f554400a9476cb9a0c3fd78"}},"3dea5104c53d408085b97c28132cc44e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43cf306d7ba3439f8179c25a6538ca2f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c2644d5aa9904ff1bad6d918313378a5","IPY_MODEL_d86544b996154fb6a930fc95d1ce41bb","IPY_MODEL_0d39af349a0a46dbb3fa7c02fb860732"],"layout":"IPY_MODEL_e4bcc6c7103a40adb9e60d7a19152da0"}},"447b2ad1dc434914a623f4660d470817":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_758e6b5fa89c4e408e5a846fdc7e85eb","max":10659,"min":0,"orientation":"horizontal","style":"IPY_MODEL_105a30493f56442eb2075f5a88d3c236","value":10659}},"47816394ccc04a25974509d2ccac120d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48cfdae9c29242b1ac7b2cec01176832":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4aecae8bfe6a4c168b5e93e453c500a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5067290722cb4a488b7c05f6194f2b3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"567a69c762354c0689ec7b21df0614ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09803a494e4642509c2a4218219c71ec","placeholder":"​","style":"IPY_MODEL_d574387da41b4d388204b523671d4067","value":"config_sentence_transformers.json: 100%"}},"56af526d68a6441aa5e229edf1e711a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5709fe89431b43849c4aebf282004392":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c698c7dfa90b41749ab1792a8f6bef99","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a21945343a3e4514a6eec0bc9bdbf15b","value":231508}},"59cdb19064a14b51ba9a20585f2e413a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59ea89db489f4ad5b940fa0eedde7eb2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a48b47e527b443882b6dd2f608a35eb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c1ee2da4423411c819bbf593e899455":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5cb9b125f1b544e78eb7c5e9e6459b6f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cc23a37be544b21ad7a09529244ec9e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6200d645526c47189685234af3799be0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bee6302dac1042798a617edf1085315a","placeholder":"​","style":"IPY_MODEL_c8ed469098584a6b9ae9c55f95530ced","value":" 350/350 [00:00&lt;00:00, 13.7kB/s]"}},"63326948732a411a881eff2fa4890743":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64050d74a49f49cdb5a7e89906bd93d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cc23a37be544b21ad7a09529244ec9e","max":349,"min":0,"orientation":"horizontal","style":"IPY_MODEL_23aba67299ab4292aed339a15eeb53c1","value":349}},"642fdc2f308549c8a5276993f7ef71e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64fd6aade5844c89858aafb7f412b1b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa877e44456e4bc1bd4727aecd01686b","placeholder":"​","style":"IPY_MODEL_722fe5d558ab42b391b0733626d4cca3","value":" 190/190 [00:00&lt;00:00, 5.25kB/s]"}},"65dbec7671e9486d99b7f532f5a54283":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d5754c825bb417083ec9b3d1a64caea","IPY_MODEL_e20bea55ceef4b0ebc8da7e42acf86e7","IPY_MODEL_64fd6aade5844c89858aafb7f412b1b0"],"layout":"IPY_MODEL_47816394ccc04a25974509d2ccac120d"}},"66578b65e9ba433780f99b751900b277":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66caf180d13b40719d2fe8d6908188cc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6abf15c7d58949348593846555ae1e15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6bf5a74c4400427a8b40c06a384809fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d5754c825bb417083ec9b3d1a64caea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8652f4a134764cfab963912450922b41","placeholder":"​","style":"IPY_MODEL_dbe07b49cf1a4f7ba83380d229230200","value":"1_Pooling/config.json: 100%"}},"6fb60459d45e4a28aa9347fd565aa58f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8112f9eccf9a44e0b437bcba1d0409ee","IPY_MODEL_447b2ad1dc434914a623f4660d470817","IPY_MODEL_b3c3c368e00d456c9fc5ea92f9d9841f"],"layout":"IPY_MODEL_b13357615f5c4505b1825cc3263d8371"}},"722fe5d558ab42b391b0733626d4cca3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"724d95e47dcb4511b7aea7f5676c2e45":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72dca2572ad042ff825d106e6ce5579b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2fdc03feb8e4d718e85e8a25d061547","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e35e5d2dd5b741068bdbf4285136dd95","value":112}},"758e6b5fa89c4e408e5a846fdc7e85eb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"778daaf6a779490e905afb3e82bf704e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc8f0bfa29b34eca803687bf97289866","max":466247,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aa8fe1bfedca456d8a3212ab3112fc64","value":466247}},"78b8f347f1d54b4684da36e39b0e493e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79cbc512027e4a36afa452baf7535c87":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bbdb56fa6f3143f69b49641df075ac7b","IPY_MODEL_cb44812f719e4e7da154549874969e9f","IPY_MODEL_b00f7d8f1f954f6ebf4cc810a0ab9d08"],"layout":"IPY_MODEL_5cb9b125f1b544e78eb7c5e9e6459b6f"}},"7eb7432e835b44c5a9794ac05772735f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59cdb19064a14b51ba9a20585f2e413a","placeholder":"​","style":"IPY_MODEL_78b8f347f1d54b4684da36e39b0e493e","value":"vocab.txt: 100%"}},"7fdc061d374b44e9b9efabfb5a847935":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8043e06e845d4b24a3d284d38f64957d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba5d70e424074f7ca727b611976acffc","IPY_MODEL_20e768d2df7e4acdbf2c83c379bc3198","IPY_MODEL_6200d645526c47189685234af3799be0"],"layout":"IPY_MODEL_8d26451d253e4dbfbc3c501e4189fe2f"}},"8059b3d85e6242c5a18f66c5bdc6a6a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8112f9eccf9a44e0b437bcba1d0409ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5d214bc49884d18abbe014f42e680a4","placeholder":"​","style":"IPY_MODEL_4aecae8bfe6a4c168b5e93e453c500a5","value":"README.md: 100%"}},"8302abbb8d39408ab51f32e9edcfe383":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"837ef3cb84b3439390b214b59848b670":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83c9865b0df34b38ab84e31e13286d28":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8652f4a134764cfab963912450922b41":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89d509e8fbef4d0884ab896f8f81f6e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8afda735d3a84c4f8bccc20590687bc8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d26451d253e4dbfbc3c501e4189fe2f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99228dda701248d38f4fdf9a2889816c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e23fac868f624ecbb27babb7a34cef05","IPY_MODEL_72dca2572ad042ff825d106e6ce5579b","IPY_MODEL_db694c49c3a0475dadca0b98f016df20"],"layout":"IPY_MODEL_21b4630774a34f7ebd3ac889d00f4a43"}},"9a9a94a4173a4b6e84f06d013351fa90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9aaebf0580754ecab53c6c9580a5dd7a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a21945343a3e4514a6eec0bc9bdbf15b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a4a969006c0743f99981ee0fb337d047":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66578b65e9ba433780f99b751900b277","placeholder":"​","style":"IPY_MODEL_b7f2e6e2621342179201daa76f325284","value":" 232k/232k [00:00&lt;00:00, 1.46MB/s]"}},"a60eb58dc44d4d8fab3c52ed3a4f2412":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa6dbe0fb9814820874e6627080b407a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8059b3d85e6242c5a18f66c5bdc6a6a2","max":116,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aab5e2ac56bc434eba045fb8295360d1","value":116}},"aa8fe1bfedca456d8a3212ab3112fc64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aa9499cb907348649972ec22e53186f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aab5e2ac56bc434eba045fb8295360d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ae7ed5c0f46a417e82089f1e6c02d824":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aff7ee3744184bcbb1630eafe8f84629":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b00f7d8f1f954f6ebf4cc810a0ab9d08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2315022199bd45f096b706fc77d845ef","placeholder":"​","style":"IPY_MODEL_5c1ee2da4423411c819bbf593e899455","value":" 612/612 [00:00&lt;00:00, 31.7kB/s]"}},"b13357615f5c4505b1825cc3263d8371":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3c3c368e00d456c9fc5ea92f9d9841f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_642fdc2f308549c8a5276993f7ef71e0","placeholder":"​","style":"IPY_MODEL_13771a3cbabc4e7d9fd707ce3d43f374","value":" 10.7k/10.7k [00:00&lt;00:00, 718kB/s]"}},"b7f2e6e2621342179201daa76f325284":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba5d70e424074f7ca727b611976acffc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d960404f08894c0f9ca5797b40f1e5f8","placeholder":"​","style":"IPY_MODEL_bcd1f7c10a6e4b3ba6e1312b877cc11d","value":"tokenizer_config.json: 100%"}},"bbdb56fa6f3143f69b49641df075ac7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ed91e4f349d4f5f942df73fd74da144","placeholder":"​","style":"IPY_MODEL_0d866581898445c29069342b81af9f59","value":"config.json: 100%"}},"bc8f0bfa29b34eca803687bf97289866":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcd1f7c10a6e4b3ba6e1312b877cc11d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bee6302dac1042798a617edf1085315a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2644d5aa9904ff1bad6d918313378a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89d509e8fbef4d0884ab896f8f81f6e8","placeholder":"​","style":"IPY_MODEL_9aaebf0580754ecab53c6c9580a5dd7a","value":"model.safetensors: 100%"}},"c4dd5e6919fe442b931ee61db75fdf95":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c698c7dfa90b41749ab1792a8f6bef99":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8ed469098584a6b9ae9c55f95530ced":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb44812f719e4e7da154549874969e9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_56af526d68a6441aa5e229edf1e711a0","max":612,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae7ed5c0f46a417e82089f1e6c02d824","value":612}},"cc2243402ff0428291c9af8679a8c745":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4dd5e6919fe442b931ee61db75fdf95","max":53,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e6905db330b8446b9ef2a3f53da2e222","value":53}},"d452a3d313004c3b82bde3b8b0a3124d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a60eb58dc44d4d8fab3c52ed3a4f2412","placeholder":"​","style":"IPY_MODEL_d72c852353d64bccb4e5e840b5f35f1e","value":" 116/116 [00:00&lt;00:00, 5.94kB/s]"}},"d574387da41b4d388204b523671d4067":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d72c852353d64bccb4e5e840b5f35f1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d86544b996154fb6a930fc95d1ce41bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_01c1a10e09c242ca8dfbeff772839f0e","max":90868376,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aff7ee3744184bcbb1630eafe8f84629","value":90868376}},"d960404f08894c0f9ca5797b40f1e5f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db694c49c3a0475dadca0b98f016df20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_162b41c35aff490c9e5e7e687ed39ca9","placeholder":"​","style":"IPY_MODEL_7fdc061d374b44e9b9efabfb5a847935","value":" 112/112 [00:00&lt;00:00, 5.40kB/s]"}},"dbe07b49cf1a4f7ba83380d229230200":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dca31c359b99468fa78672b20788fb72":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1584976b17864a84aa1369f6e09a0ecd","placeholder":"​","style":"IPY_MODEL_63326948732a411a881eff2fa4890743","value":"sentence_bert_config.json: 100%"}},"e20bea55ceef4b0ebc8da7e42acf86e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_249239bea49d4cbfa223dc978b518b72","max":190,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5067290722cb4a488b7c05f6194f2b3a","value":190}},"e23fac868f624ecbb27babb7a34cef05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a9a94a4173a4b6e84f06d013351fa90","placeholder":"​","style":"IPY_MODEL_8afda735d3a84c4f8bccc20590687bc8","value":"special_tokens_map.json: 100%"}},"e35e5d2dd5b741068bdbf4285136dd95":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4bcc6c7103a40adb9e60d7a19152da0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5d214bc49884d18abbe014f42e680a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6905db330b8446b9ef2a3f53da2e222":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f2fdc03feb8e4d718e85e8a25d061547":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa877e44456e4bc1bd4727aecd01686b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}